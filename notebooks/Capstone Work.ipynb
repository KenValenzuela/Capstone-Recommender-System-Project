{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:54:19.876046Z",
     "start_time": "2024-11-07T14:54:19.869900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Standard Library Imports\n",
    "# -----------------------------\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import random\n",
    "import csv\n",
    "import ast\n",
    "import base64\n",
    "import multiprocessing\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# -----------------------------\n",
    "# Third-Party Library Imports\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import faiss\n",
    "import redis\n",
    "import jsonpickle\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from faker import Faker\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# -----------------------------\n",
    "# Plotting and Visualization Imports\n",
    "# -----------------------------\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# -----------------------------\n",
    "# IPython and Widget Imports\n",
    "# -----------------------------\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# -----------------------------\n",
    "# NLTK and Natural Language Processing Imports\n",
    "# -----------------------------\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# -----------------------------\n",
    "# Warnings and Logging Setup\n",
    "# -----------------------------\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "# -----------------------------\n",
    "# Other Setup\n",
    "# -----------------------------\n",
    "# Additional initialization or configuration code (if any)\n"
   ],
   "id": "c5850a62c717f6fb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T19:55:09.061662Z",
     "start_time": "2024-12-21T19:55:08.852521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Load the terpene and cannabinoid info from the provided JSON files in Jupyter format\n",
    "with open('backend/data/terpene_info.json', encoding='utf-8') as terpene_file:\n",
    "    terpene_info = json.load(terpene_file)\n",
    "\n",
    "with open('/data/cannabinoid_info.json', encoding='utf-8') as cannabinoid_file:\n",
    "    cannabinoid_info = json.load(cannabinoid_file)\n",
    "\n",
    "# Assign positions, colors, and icons to cannabinoids\n",
    "cannabinoids = []\n",
    "cannabinoid_positions = np.linspace(1.5, 2.0, len(cannabinoid_info))\n",
    "for i, (name, info) in enumerate(cannabinoid_info.items()):\n",
    "    angle = 360 * i / len(cannabinoid_info)\n",
    "    x = np.cos(np.radians(angle)) * cannabinoid_positions[i]\n",
    "    y = np.sin(np.radians(angle)) * cannabinoid_positions[i]\n",
    "    cannabinoids.append({\n",
    "        'name': name,\n",
    "        'color': f'hsl({(i * 40) % 360}, 70%, 60%)',\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'icon': 'üü¢' if not info['psychoactive'] else 'üî¥'\n",
    "    })\n",
    "\n",
    "# Assign positions, colors, and icons to terpenes\n",
    "terpenes = []\n",
    "terpene_positions = np.linspace(1.2, 1.5, len(terpene_info))\n",
    "for i, (name, info) in enumerate(terpene_info.items()):\n",
    "    angle = 360 * i / len(terpene_info) + (180 / len(terpene_info))\n",
    "    x = np.cos(np.radians(angle)) * terpene_positions[i]\n",
    "    y = np.sin(np.radians(angle)) * terpene_positions[i]\n",
    "    terpenes.append({\n",
    "        'name': name,\n",
    "        'color': f'hsl({(i * 30) % 360}, 70%, 70%)',\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'icon': info.get('icon', 'üçÉ')\n",
    "    })\n",
    "\n",
    "# Create the main Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Function to add labels and hover text for terpenes and cannabinoids with enhanced layout\n",
    "def add_label(item, info_dict, label_type='terpene'):\n",
    "    name = item['name']\n",
    "    color = item['color']\n",
    "    x = item['x']\n",
    "    y = item['y']\n",
    "    icon = item['icon']\n",
    "    info = info_dict.get(name, {})\n",
    "\n",
    "    # Define hover text layout using HTML-like formatting\n",
    "    if label_type == 'terpene':\n",
    "        hovertext = (\n",
    "            f\"<b>{name}</b><br>\"\n",
    "            f\"<i>Aroma:</i> {', '.join(info.get('aroma_profile', []))}<br>\"\n",
    "            f\"<i>Effects:</i> {', '.join(info.get('effects', []))}<br>\"\n",
    "            f\"<i>Potential Benefits:</i><br>‚Ä¢ {'<br>‚Ä¢ '.join(info.get('potential_benefits', []))}<br>\"\n",
    "            f\"<i>Entourage Effect:</i><br>{info.get('entourage_effect', 'N/A')}\"\n",
    "        )\n",
    "    elif label_type == 'cannabinoid':\n",
    "        psychoactive = info.get('psychoactive', False)\n",
    "        psychoactive = 'Yes' if psychoactive else 'No'\n",
    "\n",
    "        # Enhanced hovertext layout for cannabinoids with bullet points and spacing\n",
    "        hovertext = (\n",
    "            f\"<b>{name} ({info.get('full_name', '')})</b><br>\"\n",
    "            f\"<i>Effects:</i><br>‚Ä¢ {'<br>‚Ä¢ '.join(info.get('effects', [])[:5])}...\"  # Limit to first 5 effects\n",
    "            f\"<br><i>Medical Applications:</i><br>‚Ä¢ {'<br>‚Ä¢ '.join(info.get('medical_applications', [])[:3])}...\"  # Limit to 3 applications\n",
    "            f\"<br><i>Psychoactive:</i> {psychoactive}<br>\"\n",
    "            f\"<i>Entourage Effect:</i><br>{info.get('entourage_effect', 'N/A')}\"\n",
    "        )\n",
    "\n",
    "    # Add the label to the Plotly figure\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x],\n",
    "        y=[y],\n",
    "        mode='markers+text',\n",
    "        marker=dict(\n",
    "            size=20,\n",
    "            color=color,\n",
    "            symbol='circle',\n",
    "            line=dict(width=2, color='white')\n",
    "        ),\n",
    "        text=f\"{icon}\\n{name}\",\n",
    "        textposition='top center',\n",
    "        textfont=dict(size=10, color='white', family='Arial'),\n",
    "        hoverinfo='text',\n",
    "        hovertext=hovertext,\n",
    "        name=name,\n",
    "        showlegend=False,\n",
    "        opacity=0.9\n",
    "    ))\n",
    "\n",
    "# Add terpenes and cannabinoids to the figure\n",
    "for terpene in terpenes:\n",
    "    add_label(terpene, terpene_info, label_type='terpene')\n",
    "\n",
    "for cannabinoid in cannabinoids:\n",
    "    add_label(cannabinoid, cannabinoid_info, label_type='cannabinoid')\n",
    "\n",
    "# Full-screen and layout improvements\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Understanding the Entourage Effect:<br>Greater Than The Sum of Its Parts\",\n",
    "        'y': 0.95,\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top',\n",
    "        'font': dict(size=24, color='white', family='Arial')\n",
    "    },\n",
    "    xaxis=dict(\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showticklabels=False,\n",
    "        range=[-2.5, 2.5]\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showticklabels=False,\n",
    "        range=[-2.5, 2.5],\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1\n",
    "    ),\n",
    "    paper_bgcolor='rgba(30, 30, 30, 1)',\n",
    "    plot_bgcolor='rgba(30, 30, 30, 1)',\n",
    "    width=900,\n",
    "    height=900,\n",
    "    margin=dict(l=50, r=50, t=150, b=50),\n",
    "    font=dict(color='white', family='Arial'),\n",
    "    updatemenus=[\n",
    "        {\n",
    "            \"buttons\": [\n",
    "                {\n",
    "                    \"args\": [{\"fullscreen\": True}],\n",
    "                    \"label\": \"Full Screen\",\n",
    "                    \"method\": \"relayout\",\n",
    "                }\n",
    "            ],\n",
    "            \"direction\": \"left\",\n",
    "            \"pad\": {\"r\": 10, \"t\": 10},\n",
    "            \"showactive\": False,\n",
    "            \"type\": \"buttons\",\n",
    "            \"x\": 0.0,\n",
    "            \"xanchor\": \"left\",\n",
    "            \"y\": 1.2,\n",
    "            \"yanchor\": \"top\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add a descriptive annotation about the entourage effect\n",
    "fig.add_annotation(\n",
    "    x=0,\n",
    "    y=-2.2,\n",
    "    text=(\n",
    "        \"<b>The Entourage Effect</b><br>\"\n",
    "        \"The Entourage Effect refers to the synergistic interaction between various cannabinoids and terpenes, \"\n",
    "        \"suggesting that these compounds work together to produce a more pronounced therapeutic effect \"\n",
    "        \"than any single compound alone.<br><i>Hover over the compounds to learn more about each one.</i>\"\n",
    "    ),\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color='white', family='Arial'),\n",
    "    align='center',\n",
    "    bordercolor='white',\n",
    "    borderwidth=1,\n",
    "    bgcolor='rgba(42, 42, 42, 0.8)',\n",
    "    xanchor='center',\n",
    "    yanchor='top'\n",
    ")\n",
    "\n",
    "# Add the static background image (ensure image path is correct)\n",
    "with open(\".venv/Weedimage.png\", \"rb\") as image_file:\n",
    "    encoded_image = base64.b64encode(image_file.read()).decode()\n",
    "\n",
    "fig.update_layout(\n",
    "    images=[\n",
    "        dict(\n",
    "            source='data:image/png;base64,{}'.format(encoded_image),\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            x=0,\n",
    "            y=1,\n",
    "            sizex=1,\n",
    "            sizey=1,\n",
    "            sizing=\"stretch\",\n",
    "            opacity=0.5,\n",
    "            layer=\"below\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n"
   ],
   "id": "5504548803dd6b2d",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'backend/data/terpene_info.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load the terpene and cannabinoid info from the provided JSON files in Jupyter format\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbackend/data/terpene_info.json\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m terpene_file:\n\u001B[1;32m      3\u001B[0m     terpene_info \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(terpene_file)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/data/cannabinoid_info.json\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m cannabinoid_file:\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    322\u001B[0m     )\n\u001B[0;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'backend/data/terpene_info.json'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T19:55:10.671044Z",
     "start_time": "2024-12-21T19:55:10.618729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "# Suppress warnings from PyTorch and Transformers\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torch\").setLevel(logging.ERROR)\n",
    "\n",
    "# Ensure that progress_apply is properly registered\n",
    "tqdm.pandas()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load terpene and cannabinoid info from files\n",
    "# -----------------------------\n",
    "def load_json(filepath: str):\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        logging.warning(f\"{filepath} not found, loading empty dictionary.\")\n",
    "        return {}\n",
    "\n",
    "terpene_info = load_json('.venv/terpene_info.json')\n",
    "cannabinoid_info = load_json('.venv/cannabinoid_info.json')\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Define regular expressions and synonym cache \n",
    "# -----------------------------\n",
    "THC_REGEX = re.compile(r'thc[a-z]*:\\s*([\\d.]+)%', re.IGNORECASE)\n",
    "CBD_REGEX = re.compile(r'cbd[a-z]*:\\s*([\\d.]+)%', re.IGNORECASE)\n",
    "HTML_TAGS_REGEX = re.compile(r'<br>|<[^>]+>')\n",
    "NON_ASCII_REGEX = re.compile(r'[^\\x00-\\x7F]+')\n",
    "NON_ALPHABETIC_REGEX = re.compile(r'[^a-zA-Z\\s]')\n",
    "PIPE_COMMA_REGEX = re.compile(r'\\||,')\n",
    "UNWANTED_ENTRIES = {'unknown', 'not yet listed', 'not available'}\n",
    "EXTRA_SPACES_REGEX = re.compile(r'\\s+')\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_synonyms(word: str) -> set:\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        synonyms.update(lemma.name().replace('_', ' ').lower() for lemma in syn.lemmas())\n",
    "    synonyms.discard(word.lower())\n",
    "    return synonyms\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Define data cleaning functions\n",
    "# -----------------------------\n",
    "def clean_description_series(text_series: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        text_series.astype(str)\n",
    "        .str.replace(HTML_TAGS_REGEX, '', regex=True)\n",
    "        .str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "        .str.replace(NON_ALPHABETIC_REGEX, ' ', regex=True)\n",
    "        .str.lower().str.strip()\n",
    "        .str.replace(EXTRA_SPACES_REGEX, ' ', regex=True)\n",
    "    )\n",
    "\n",
    "def clean_pipe_columns_series(text_series: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        text_series.astype(str)\n",
    "        .str.split(PIPE_COMMA_REGEX)\n",
    "        .apply(lambda parts: ', '.join(sorted(\n",
    "            set(part.strip() for part in parts if part.strip().lower() not in UNWANTED_ENTRIES and part.strip())\n",
    "        )) if parts else 'Unknown')\n",
    "    )\n",
    "\n",
    "def standardize_type_series(type_series: pd.Series) -> pd.Series:\n",
    "    type_series = type_series.astype(str).str.lower().str.strip()\n",
    "    return np.select(\n",
    "        [\n",
    "            type_series.str.contains('hybrid'),\n",
    "            type_series.str.contains('indica'),\n",
    "            type_series.str.contains('sativa')\n",
    "        ],\n",
    "        ['Hybrid', 'Indica', 'Sativa'],\n",
    "        default='Unknown'\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Define mappings for cannabinoids and terpenes\n",
    "# -----------------------------\n",
    "flavor_aroma_to_terpene = defaultdict(set)\n",
    "cannabinoid_synonyms = {}\n",
    "\n",
    "# Process terpenes\n",
    "for terpene, info in tqdm(terpene_info.items(), desc='Processing Terpenes'):\n",
    "    for flavor in info.get('taste_profile', []):\n",
    "        flavor_lower = flavor.lower()\n",
    "        flavor_aroma_to_terpene[flavor_lower].add(terpene)\n",
    "        for syn in get_synonyms(flavor_lower):\n",
    "            flavor_aroma_to_terpene[syn].add(terpene)\n",
    "\n",
    "# Process cannabinoids\n",
    "for cannabinoid, info in tqdm(cannabinoid_info.items(), desc='Processing Cannabinoids'):\n",
    "    cannabinoid_lower = cannabinoid.lower()\n",
    "    synonyms = get_synonyms(cannabinoid_lower)\n",
    "    synonyms.add(cannabinoid_lower)\n",
    "    cannabinoid_synonyms[cannabinoid_lower] = synonyms\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Define mapping functions\n",
    "# -----------------------------\n",
    "def map_cannabinoids(description: str) -> str:\n",
    "    all_cannabinoids = set(cannabinoid_info.keys())\n",
    "    if pd.isnull(description) or not description.strip():\n",
    "        return ', '.join(sorted(all_cannabinoids))\n",
    "    tokens = set(description.lower().split())\n",
    "    cannabinoids_found = {cannabinoid.title() for cannabinoid, synonyms in cannabinoid_synonyms.items() if tokens & synonyms}\n",
    "    return ', '.join(sorted(cannabinoids_found)) if cannabinoids_found else ', '.join(sorted(all_cannabinoids))\n",
    "\n",
    "def map_terpenes(description: str) -> str:\n",
    "    if pd.isnull(description) or not description.strip():\n",
    "        return 'Unknown'\n",
    "    tokens = set(description.lower().split())\n",
    "    terpenes_found = set()\n",
    "    for word in tokens:\n",
    "        terpenes_found.update(flavor_aroma_to_terpene.get(word, set()))\n",
    "    return ', '.join(sorted(terpenes_found)) if terpenes_found else 'Unknown'\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Load strain data and apply cleaning functions\n",
    "# -----------------------------\n",
    "def clean_strain_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['Strain Description'] = clean_description_series(df['Strain Description'])\n",
    "    df.drop_duplicates(subset=['Strain_Name', 'Strain Description'], inplace=True)\n",
    "    columns_to_clean = ['Flavors', 'Aromas', 'May Relieve', 'Effects']\n",
    "    for column in columns_to_clean:\n",
    "        if column in df.columns:\n",
    "            df[column] = clean_pipe_columns_series(df[column])\n",
    "        else:\n",
    "            df[column] = 'Unknown'\n",
    "    if 'Type' in df.columns:\n",
    "        df['Type'] = standardize_type_series(df['Type'])\n",
    "    else:\n",
    "        df['Type'] = 'Unknown'\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['strain_id'] = df.index  # Ensure strain_id is an integer\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Step 7: Process mappings and create additional fields\n",
    "# -----------------------------\n",
    "file_path = '.venv/cleaned_and_reordered_data.csv'\n",
    "strain_dataframe = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='skip')\n",
    "strain_dataframe_cleaned = clean_strain_dataframe(strain_dataframe)\n",
    "\n",
    "tqdm.pandas(desc=\"Applying Mapping Functions\")\n",
    "strain_dataframe_cleaned['Cannabinoid Profile'] = strain_dataframe_cleaned['Strain Description'].progress_apply(map_cannabinoids)\n",
    "strain_dataframe_cleaned['Terpene Profile'] = strain_dataframe_cleaned['Strain Description'].progress_apply(map_terpenes)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 8: MultiLabelBinarizer for Categorical Data\n",
    "# -----------------------------\n",
    "def apply_multilabel_binarizer(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df[column] = df[column].astype(str).apply(\n",
    "        lambda x: [item.strip().lower() for item in x.split(\", \") if item.strip().lower() != 'unknown']\n",
    "    )\n",
    "    transformed = mlb.fit_transform(df[column])\n",
    "    return pd.DataFrame(transformed, columns=[f\"{column}_{cls}\" for cls in mlb.classes_], index=df.index)\n",
    "\n",
    "categorical_columns = ['Flavors', 'Aromas', 'Effects', 'May Relieve', 'Type', 'Cannabinoid Profile', 'Terpene Profile']\n",
    "\n",
    "binarized_dfs = {}\n",
    "for column in tqdm(categorical_columns, desc=\"Applying MultiLabelBinarizer\"):\n",
    "    binarized_dfs[column] = apply_multilabel_binarizer(strain_dataframe_cleaned, column)\n",
    "\n",
    "strain_dataframe_encoded = pd.concat([strain_dataframe_cleaned] + list(binarized_dfs.values()), axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 9: Drop Redundant Columns and Final Clean-up\n",
    "# -----------------------------\n",
    "columns_to_drop = categorical_columns + ['Strain Description']\n",
    "strain_dataframe_final = strain_dataframe_encoded.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Pre-emptively handle inf and NaN values\n",
    "strain_dataframe_final.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "strain_dataframe_final.fillna(0, inplace=True)\n",
    "\n",
    "# Verify that strain_id remains an integer\n",
    "strain_dataframe_final['strain_id'] = strain_dataframe_final['strain_id'].astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 10: Generate BERT Embeddings (Optimized)\n",
    "# -----------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def get_bert_embeddings(texts, tokenizer, model, device, max_length=128, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    progress_bar = tqdm(total=len(texts), desc=\"Generating BERT Embeddings\", unit=\"text\", leave=True)\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Ensure all texts are strings\n",
    "        batch_texts = [str(text) for text in batch_texts]\n",
    "        \n",
    "        # Batch tokenize the input texts\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=max_length, \n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_input)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token embedding\n",
    "\n",
    "        all_embeddings.extend(embeddings)\n",
    "        progress_bar.update(len(batch_texts))\n",
    "    \n",
    "    progress_bar.close()\n",
    "    return np.array(all_embeddings, dtype=np.float32)\n",
    "\n",
    "# Ensure that the texts are correctly formatted\n",
    "texts = strain_dataframe_cleaned['Strain Description'].fillna('').astype(str).tolist()\n",
    "\n",
    "# Generate BERT embeddings\n",
    "bert_embeddings = get_bert_embeddings(\n",
    "    texts,\n",
    "    tokenizer, \n",
    "    bert_model, \n",
    "    device, \n",
    "    max_length=128,  \n",
    "    batch_size=64    \n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 11: Cosine Similarity Matrix and Save Output\n",
    "# -----------------------------\n",
    "cosine_sim_matrix = cosine_similarity(bert_embeddings)\n",
    "output_file_path = '.venv/cleaned_strain_data_final_with_embeddings.csv'\n",
    "\n",
    "# Save the embeddings into the dataframe\n",
    "embedding_columns = [f\"embedding_{i}\" for i in range(bert_embeddings.shape[1])]\n",
    "embeddings_df = pd.DataFrame(bert_embeddings, columns=embedding_columns)\n",
    "strain_dataframe_final_with_embeddings = pd.concat([strain_dataframe_final.reset_index(drop=True), embeddings_df], axis=1)\n",
    "\n",
    "strain_dataframe_final_with_embeddings.to_csv(output_file_path, index=False)\n",
    "\n",
    "cosine_similarity_output_path = '.venv/strain_cosine_similarity_matrix.csv'\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=strain_dataframe_cleaned['strain_id'], columns=strain_dataframe_cleaned['strain_id'])\n",
    "cosine_sim_df.to_csv(cosine_similarity_output_path)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 12: FAISS for Top-N Similarities\n",
    "# -----------------------------\n",
    "# Normalize embeddings for cosine similarity\n",
    "faiss.normalize_L2(bert_embeddings)\n",
    "index = faiss.IndexFlatIP(bert_embeddings.shape[1])\n",
    "index.add(bert_embeddings)\n",
    "\n",
    "TOP_N = 10\n",
    "distances, indices = index.search(bert_embeddings, TOP_N + 1)\n",
    "similarity_data = []\n",
    "\n",
    "for i in tqdm(range(len(strain_dataframe_cleaned)), desc=\"Finding Top-N Similarities\"):\n",
    "    strain_id = strain_dataframe_cleaned['strain_id'].iloc[i]\n",
    "    for j in range(1, TOP_N + 1):\n",
    "        similar_strain_id = strain_dataframe_cleaned['strain_id'].iloc[indices[i][j]]\n",
    "        similarity_score = distances[i][j]\n",
    "        similarity_data.append({\n",
    "            'strain_id': strain_id,\n",
    "            'similar_strain_id': similar_strain_id,\n",
    "            'similarity_score': similarity_score\n",
    "        })\n",
    "\n",
    "similarity_output_path = '.venv/strain_cosine_similarity_matrix_top10.csv'\n",
    "pd.DataFrame(similarity_data).to_csv(similarity_output_path, index=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Final confirmation of results\n",
    "# -----------------------------\n",
    "print(strain_dataframe_final_with_embeddings.tail())\n",
    "\n",
    "# Ensure any remaining tensor resources are cleaned up\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "309c50fa73995ad0",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'warnings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lru_cache\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Suppress warnings from PyTorch and Transformers\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mwarnings\u001B[49m\u001B[38;5;241m.\u001B[39mfilterwarnings(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m, category\u001B[38;5;241m=\u001B[39m\u001B[38;5;167;01mUserWarning\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransformers\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msetLevel(logging\u001B[38;5;241m.\u001B[39mERROR)\n\u001B[1;32m      6\u001B[0m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msetLevel(logging\u001B[38;5;241m.\u001B[39mERROR)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'warnings' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T19:55:11.826752Z",
     "start_time": "2024-12-21T19:55:11.458002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import faker\n",
    "\n",
    "CONFIG = {\n",
    "    'num_profiles': 150000,\n",
    "    'min_ratings_per_user': 50,\n",
    "    'max_ratings_per_user': 300,\n",
    "    'total_ratings': 20000000,\n",
    "    'personas_filepath': '.venv/user_profiles_template.csv',\n",
    "    'reviews_filepath': '.venv/review_templates.csv',\n",
    "    'output_filepath': '.venv/synthetic_profiles_with_reviews.csv',\n",
    "    'strain_data_filepath': '.venv/cleaned_strain_data_final.csv',\n",
    "    'chemical_data': {\n",
    "        'terpene_info': '.venv/terpene_info.json',\n",
    "        'cannabinoid_info': '.venv/cannabinoid_info.json'\n",
    "    },\n",
    "    'redis': {\n",
    "        'host': 'localhost',\n",
    "        'port': 6379,\n",
    "        'db': 0,\n",
    "        'retries': 5\n",
    "    },\n",
    "    'logging': {\n",
    "        'level': 'INFO',\n",
    "        'file': '.venv/synthetic_data_generation.log'\n",
    "    },\n",
    "    'parallel': {\n",
    "        'processes': multiprocessing.cpu_count(),\n",
    "        'chunk_size': 1000\n",
    "    }\n",
    "}\n",
    "\n",
    "def configure_logging(log_level: str, log_file: str):\n",
    "    numeric_level = getattr(logging, log_level.upper(), None)\n",
    "    if not isinstance(numeric_level, int):\n",
    "        numeric_level = logging.WARNING\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=numeric_level,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "configure_logging(CONFIG['logging']['level'], CONFIG['logging']['file'])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "fake = Faker()\n",
    "fake_unique = Faker()\n",
    "fake_unique.unique.clear()\n",
    "\n",
    "def download_nltk_data():\n",
    "    try:\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        nltk.download('omw-1.4', quiet=True)\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "download_nltk_data()\n",
    "\n",
    "def load_strain_data(filepath: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Strain data file not found: {filepath}\")\n",
    "\n",
    "        strain_dataframe = pd.read_csv(filepath)\n",
    "\n",
    "        if 'strain_id' not in strain_dataframe.columns:\n",
    "            strain_dataframe.reset_index(drop=True, inplace=True)\n",
    "            strain_dataframe['strain_id'] = strain_dataframe.index + 1\n",
    "\n",
    "        strain_dataframe.drop_duplicates(subset='strain_id', inplace=True)\n",
    "\n",
    "        return strain_dataframe\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        raise\n",
    "    except pd.errors.ParserError as e:\n",
    "        raise\n",
    "\n",
    "strain_data = load_strain_data(CONFIG['strain_data_filepath'])\n",
    "strain_data_list = strain_data[['strain_id', 'Strain_Name']].to_dict('records')\n",
    "strain_name_to_id = {row['Strain_Name']: row['strain_id'] for row in strain_data_list}\n",
    "\n",
    "def initialize_redis(host: str, port: int, db: int, retries: int):\n",
    "    retry_count = 0\n",
    "    while retry_count < retries:\n",
    "        try:\n",
    "            pool = redis.ConnectionPool(host=host, port=port, db=db, decode_responses=True)\n",
    "            cache = redis.StrictRedis(connection_pool=pool)\n",
    "            cache.ping()\n",
    "            return cache\n",
    "        except redis.ConnectionError:\n",
    "            retry_count += 1\n",
    "    return None\n",
    "\n",
    "redis_client = initialize_redis(\n",
    "    host=CONFIG['redis']['host'],\n",
    "    port=CONFIG['redis']['port'],\n",
    "    db=CONFIG['redis']['db'],\n",
    "    retries=CONFIG['redis']['retries']\n",
    ")\n",
    "\n",
    "def load_json_file(filepath: str) -> Dict:\n",
    "    try:\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError as e:\n",
    "        raise\n",
    "    except json.JSONDecodeError:\n",
    "        raise\n",
    "\n",
    "def validate_terpene_info(data: Dict):\n",
    "    required_keys = {'taste_profile', 'effects'}\n",
    "    for terp, details in data.items():\n",
    "        if not required_keys.issubset(details.keys()):\n",
    "            missing = required_keys - set(details.keys())\n",
    "            raise ValueError(f\"Terpene '{terp}' is missing required keys: {missing}\")\n",
    "\n",
    "def load_chemical_data(terpene_filepath: str, cannabinoid_filepath: str) -> Dict:\n",
    "    terpene_info = load_json_file(terpene_filepath)\n",
    "    validate_terpene_info(terpene_info)\n",
    "    cannabinoid_info = load_json_file(cannabinoid_filepath)\n",
    "    return {'terpenes': terpene_info, 'cannabinoids': cannabinoid_info}\n",
    "\n",
    "chemical_data = load_chemical_data(\n",
    "    CONFIG['chemical_data']['terpene_info'],\n",
    "    CONFIG['chemical_data']['cannabinoid_info']\n",
    ")\n",
    "\n",
    "def extract_attributes(chemical_data: Dict) -> Dict:\n",
    "    all_terpenes = list(chemical_data['terpenes'].keys())\n",
    "    all_flavors = list(set(\n",
    "        flavor.lower()\n",
    "        for terpene in chemical_data['terpenes'].values()\n",
    "        for flavor in terpene.get('taste_profile', [])\n",
    "    ))\n",
    "    all_effects = list(set(\n",
    "        effect.lower()\n",
    "        for terpene in chemical_data['terpenes'].values()\n",
    "        for effect in terpene.get('effects', [])\n",
    "    ))\n",
    "    return {\n",
    "        'terpenes': all_terpenes,\n",
    "        'flavors': all_flavors,\n",
    "        'effects': all_effects\n",
    "    }\n",
    "\n",
    "extracted_attributes = extract_attributes(chemical_data)\n",
    "\n",
    "def encode_categorical_features(df: pd.DataFrame, categorical_columns: List[str]) -> pd.DataFrame:\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_features = encoder.fit_transform(df[categorical_columns])\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "    encoded_df.index = df.index\n",
    "    df = df.drop(columns=categorical_columns)\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    return df\n",
    "\n",
    "defined_attributes = {\n",
    "    'available_states': ['California', 'Colorado', 'Oregon', 'Washington', 'Nevada', 'Arizona'],\n",
    "    'usage_frequencies': ['Daily', 'Weekly', 'Monthly', 'Occasionally'],\n",
    "    'strain_types': ['Indica', 'Sativa', 'Hybrid'],\n",
    "    'consumption_methods': ['Smoking', 'Vaporizing', 'Edibles', 'Tinctures', 'Topicals'],\n",
    "    'product_type_preferences': ['Edibles', 'Flower', 'Tinctures', 'Concentrates', 'Capsules'],\n",
    "    'consumption_times': ['Morning', 'Afternoon', 'Evening', 'Night'],\n",
    "    'social_contexts': ['Alone', 'With Friends', 'Social Gatherings'],\n",
    "    'genders': ['male', 'female', 'non-binary', 'other']\n",
    "}\n",
    "\n",
    "def load_and_expand_personas(filepath: str, attributes: Dict, min_ratings: int, max_ratings: int, total_ratings: int) -> pd.DataFrame:\n",
    "    try:\n",
    "        if os.path.exists(filepath):\n",
    "            personas = pd.read_csv(filepath)\n",
    "        else:\n",
    "            personas = pd.DataFrame()\n",
    "\n",
    "        fake_unique = Faker()\n",
    "        fake_unique.unique.clear()\n",
    "\n",
    "        required_columns = ['user_id', 'user_name', 'age', 'gender', 'location', 'usage_frequency',\n",
    "                            'consumption_time', 'social_context', 'preferred_strain_type',\n",
    "                            'product_type_preference', 'num_ratings']\n",
    "        for col in required_columns:\n",
    "            if col not in personas.columns:\n",
    "                if col == 'user_id':\n",
    "                    personas['user_id'] = range(1, len(personas) + 1)\n",
    "                elif col == 'user_name':\n",
    "                    personas['user_name'] = [fake_unique.unique.name() for _ in range(len(personas))]\n",
    "                elif col == 'num_ratings':\n",
    "                    personas['num_ratings'] = [random.randint(min_ratings, max_ratings) for _ in range(len(personas))]\n",
    "                else:\n",
    "                    key = col + 's' if col + 's' in attributes else col\n",
    "                    if key in attributes:\n",
    "                        personas[col] = [random.choice(attributes[key]) for _ in range(len(personas))]\n",
    "                    else:\n",
    "                        personas[col] = 'Unknown'\n",
    "\n",
    "        if 'num_ratings' in personas.columns:\n",
    "            personas['num_ratings'] = personas['num_ratings'].apply(\n",
    "                lambda x: random.randint(min_ratings, max_ratings) if pd.isnull(x) else int(x)\n",
    "            ).astype(int)\n",
    "\n",
    "        additional_personas = []\n",
    "        current_count = len(personas)\n",
    "\n",
    "        generated_ratings = personas['num_ratings'].sum()\n",
    "        user_id_counter = personas['user_id'].max() + 1 if current_count > 0 else 1\n",
    "\n",
    "        while generated_ratings < total_ratings:\n",
    "            try:\n",
    "                user_name = fake_unique.unique.name()\n",
    "            except faker.exceptions.UniquenessException:\n",
    "                fake_unique.unique.clear()\n",
    "                user_name = fake_unique.unique.name()\n",
    "\n",
    "            num_ratings = random.randint(min_ratings, max_ratings)\n",
    "\n",
    "            if generated_ratings + num_ratings > total_ratings:\n",
    "                num_ratings = total_ratings - generated_ratings\n",
    "\n",
    "            generated_ratings += num_ratings\n",
    "\n",
    "            additional_personas.append({\n",
    "                'user_id': user_id_counter,\n",
    "                'user_name': user_name,\n",
    "                'age': random.randint(18, 65),\n",
    "                'gender': random.choice(attributes['genders']),\n",
    "                'location': random.choice(attributes['available_states']),\n",
    "                'usage_frequency': random.choice(attributes['usage_frequencies']),\n",
    "                'consumption_time': random.choice(attributes['consumption_times']),\n",
    "                'social_context': random.choice(attributes['social_contexts']),\n",
    "                'preferred_strain_type': random.choice(attributes['strain_types']),\n",
    "                'product_type_preference': random.choice(attributes['product_type_preferences']),\n",
    "                'num_ratings': num_ratings\n",
    "            })\n",
    "\n",
    "            user_id_counter += 1\n",
    "\n",
    "        additional_personas_df = pd.DataFrame(additional_personas)\n",
    "        personas = pd.concat([personas, additional_personas_df], ignore_index=True)\n",
    "\n",
    "        categorical_columns = ['preferred_strain_type', 'consumption_time', 'usage_frequency', 'product_type_preference', 'social_context', 'gender']\n",
    "        personas = encode_categorical_features(personas, categorical_columns)\n",
    "\n",
    "        return personas\n",
    "    except FileNotFoundError as e:\n",
    "        raise\n",
    "    except pd.errors.ParserError as e:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "personas = load_and_expand_personas(\n",
    "    filepath=CONFIG['personas_filepath'],\n",
    "    attributes=defined_attributes,\n",
    "    min_ratings=CONFIG['min_ratings_per_user'],\n",
    "    max_ratings=CONFIG['max_ratings_per_user'],\n",
    "    total_ratings=CONFIG['total_ratings']\n",
    ")\n",
    "\n",
    "def load_review_templates(filepath: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Review templates file not found: {filepath}\")\n",
    "\n",
    "        reviews_df = pd.read_csv(filepath)\n",
    "        return reviews_df\n",
    "    except FileNotFoundError as e:\n",
    "        raise\n",
    "    except pd.errors.ParserError as e:\n",
    "        raise\n",
    "\n",
    "reviews_df = load_review_templates(CONFIG['reviews_filepath'])\n",
    "\n",
    "def get_synonym(word: str) -> str:\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    if synonyms:\n",
    "        synonym = synonyms[0].lemmas()[0].name()\n",
    "        if synonym.lower() != word.lower():\n",
    "            return synonym.replace('_', ' ')\n",
    "    return word\n",
    "\n",
    "def augment_review_text(review_text: str) -> str:\n",
    "    words = review_text.split()\n",
    "    augmented = []\n",
    "    for word in words:\n",
    "        if random.random() < 0.2:\n",
    "            synonym = get_synonym(word)\n",
    "            augmented.append(synonym if synonym else word)\n",
    "        else:\n",
    "            augmented.append(word)\n",
    "    return ' '.join(augmented)\n",
    "\n",
    "intro_templates = [\n",
    "    \"I recently tried {strain_name} using {consumption_method}.\",\n",
    "    \"My experience with {strain_name} through {consumption_method} was quite {adjective}.\",\n",
    "    \"Using {consumption_method}, I found {strain_name} to be {adjective}.\",\n",
    "    \"After using {consumption_method} with {strain_name}, I felt {feeling}.\"\n",
    "]\n",
    "\n",
    "body_templates = [\n",
    "    \"{rating_comment}\",\n",
    "    \"The effects were {terpene_effects} and {entourage_effect}.\",\n",
    "    \"I felt {terpene_effects}, which made the experience {experience_description}.\",\n",
    "    \"Overall, the strain provided {entourage_effect} effects.\"\n",
    "]\n",
    "\n",
    "conclusion_templates = [\n",
    "    \"It had a distinct {taste_profile} flavor, which I found very enjoyable.\",\n",
    "    \"The {taste_profile} taste complemented the effects well.\",\n",
    "    \"The flavor was {taste_profile}, enhancing the overall experience.\",\n",
    "    \"I particularly liked the {taste_profile} flavor.\"\n",
    "]\n",
    "\n",
    "adjectives = ['pleasant', 'intense', 'mellow', 'strong', 'subtle']\n",
    "feelings = ['relaxed', 'energized', 'uplifted', 'calm']\n",
    "experience_descriptions = ['unforgettable', 'soothing', 'refreshing', 'invigorating']\n",
    "\n",
    "def generate_combined_review(strain_name: str, consumption_method: str, rating_comment: str,\n",
    "                             terpene_effects: str, entourage_effect: str, taste_profile: str) -> str:\n",
    "    intro = random.choice(intro_templates).format(\n",
    "        strain_name=strain_name,\n",
    "        consumption_method=consumption_method,\n",
    "        adjective=random.choice(adjectives),\n",
    "        feeling=random.choice(feelings)\n",
    "    )\n",
    "    body = random.choice(body_templates).format(\n",
    "        rating_comment=rating_comment,\n",
    "        terpene_effects=terpene_effects,\n",
    "        entourage_effect=entourage_effect,\n",
    "        experience_description=random.choice(experience_descriptions)\n",
    "    )\n",
    "    conclusion = random.choice(conclusion_templates).format(\n",
    "        taste_profile=taste_profile\n",
    "    )\n",
    "    return f\"{intro} {body} {conclusion}\"\n",
    "\n",
    "def calculate_sentiment(rating: int, comment: str) -> float:\n",
    "    try:\n",
    "        analysis = TextBlob(comment)\n",
    "        sentiment_polarity = analysis.sentiment.polarity\n",
    "        adjusted_sentiment = sentiment_polarity + ((rating - 3) * 0.1)\n",
    "        return max(min(adjusted_sentiment, 1.0), -1.0)\n",
    "    except Exception as e:\n",
    "        return 0.0\n",
    "\n",
    "def generate_unique_cache_key(user_id: int, strain_id: int) -> str:\n",
    "    return f\"{user_id}_{strain_id}\"\n",
    "\n",
    "def main():\n",
    "    global strain_data_list, reviews_df, extracted_attributes, defined_attributes, strain_name_to_id\n",
    "\n",
    "    attributes = {**extracted_attributes, **defined_attributes}\n",
    "\n",
    "    strain_list = strain_data_list\n",
    "\n",
    "    personas['num_ratings'] = personas['num_ratings'].apply(\n",
    "        lambda x: random.randint(CONFIG['min_ratings_per_user'], CONFIG['max_ratings_per_user']) if pd.isnull(x) else int(x)\n",
    "    ).astype(int)\n",
    "\n",
    "    total_reviews = personas['num_ratings'].sum()\n",
    "\n",
    "    with open(CONFIG['output_filepath'], 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['user_id', 'user_name', 'strain_name', 'strain_id', 'rating', 'rating_comment', 'side_effects', 'taste_profile', 'review', 'sentiment_score']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        chunk_size = CONFIG['parallel']['chunk_size']\n",
    "        persona_chunks = [personas.iloc[i:i + chunk_size] for i in range(0, len(personas), chunk_size)]\n",
    "\n",
    "        for chunk_index, persona_chunk in enumerate(persona_chunks):\n",
    "            tasks = []\n",
    "            for _, row in persona_chunk.iterrows():\n",
    "                profile_dict = row.to_dict()\n",
    "                tasks.append(profile_dict)\n",
    "\n",
    "            with ProcessPoolExecutor(max_workers=CONFIG['parallel']['processes']) as executor:\n",
    "                futures = {executor.submit(process_user_reviews, profile): profile['user_id'] for profile in tasks}\n",
    "\n",
    "                for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing Chunk {chunk_index + 1}\"):\n",
    "                    user_id = futures[future]\n",
    "                    try:\n",
    "                        user_reviews = future.result()\n",
    "                        if user_reviews:\n",
    "                            for review in user_reviews:\n",
    "                                writer.writerow(review)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "\n",
    "def process_user_reviews(profile: Dict) -> List[Dict]:\n",
    "    user_reviews = []\n",
    "    try:\n",
    "        num_ratings = int(profile['num_ratings'])\n",
    "        user_id = profile.get('user_id', 'unknown')\n",
    "\n",
    "        worker_redis_client = None\n",
    "\n",
    "        for _ in range(num_ratings):\n",
    "            strain = random.choice(strain_data_list)\n",
    "            review = generate_synthetic_review(profile, strain, worker_redis_client)\n",
    "            if review:\n",
    "                user_reviews.append(review)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return user_reviews\n",
    "\n",
    "def generate_synthetic_review(profile: Dict, strain: Dict, worker_redis_client=None) -> Dict:\n",
    "    try:\n",
    "        strain_name = strain.get('Strain_Name', 'Unknown Strain')\n",
    "        strain_id = strain.get('strain_id') or strain_name_to_id.get(strain_name, None)\n",
    "\n",
    "        if strain_id is None:\n",
    "            return {}\n",
    "\n",
    "        user_id = profile['user_id']\n",
    "\n",
    "        selected_review = reviews_df.sample(1).iloc[0]\n",
    "        consumption_method = random.choice(defined_attributes['consumption_methods'])\n",
    "        sentiment_score = calculate_sentiment(selected_review['rating'], selected_review['rating_comment'])\n",
    "\n",
    "        review_text = generate_combined_review(\n",
    "            strain_name=strain_name,\n",
    "            consumption_method=consumption_method,\n",
    "            rating_comment=selected_review['rating_comment'],\n",
    "            terpene_effects=random.choice(extracted_attributes['effects']),\n",
    "            entourage_effect=random.choice(extracted_attributes['effects']),\n",
    "            taste_profile=selected_review['taste_profile']\n",
    "        )\n",
    "\n",
    "        review_text = augment_review_text(review_text)\n",
    "\n",
    "        review_dict = {\n",
    "            'user_id': user_id,\n",
    "            'user_name': profile['user_name'],\n",
    "            'strain_name': strain_name,\n",
    "            'strain_id': strain_id,\n",
    "            'rating': int(selected_review['rating']),\n",
    "            'rating_comment': selected_review['rating_comment'],\n",
    "            'side_effects': selected_review.get('side_effects', ''),\n",
    "            'taste_profile': selected_review['taste_profile'],\n",
    "            'review': review_text,\n",
    "            'sentiment_score': sentiment_score\n",
    "        }\n",
    "\n",
    "        return review_dict\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "eae7b7ccd05e6529",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multiprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 27\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfaker\u001B[39;00m\n\u001B[1;32m      3\u001B[0m CONFIG \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_profiles\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m150000\u001B[39m,\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin_ratings_per_user\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m50\u001B[39m,\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_ratings_per_user\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m300\u001B[39m,\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtotal_ratings\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m20000000\u001B[39m,\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpersonas_filepath\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.venv/user_profiles_template.csv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreviews_filepath\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.venv/review_templates.csv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput_filepath\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.venv/synthetic_profiles_with_reviews.csv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrain_data_filepath\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.venv/cleaned_strain_data_final.csv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchemical_data\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mterpene_info\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.venv/terpene_info.json\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcannabinoid_info\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.venv/cannabinoid_info.json\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     15\u001B[0m     },\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mredis\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     17\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhost\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocalhost\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mport\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m6379\u001B[39m,\n\u001B[1;32m     19\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdb\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mretries\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m5\u001B[39m\n\u001B[1;32m     21\u001B[0m     },\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     23\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlevel\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mINFO\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     24\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfile\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.venv/synthetic_data_generation.log\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     25\u001B[0m     },\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparallel\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocesses\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43mmultiprocessing\u001B[49m\u001B[38;5;241m.\u001B[39mcpu_count(),\n\u001B[1;32m     28\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchunk_size\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1000\u001B[39m\n\u001B[1;32m     29\u001B[0m     }\n\u001B[1;32m     30\u001B[0m }\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconfigure_logging\u001B[39m(log_level: \u001B[38;5;28mstr\u001B[39m, log_file: \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m     33\u001B[0m     numeric_level \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(logging, log_level\u001B[38;5;241m.\u001B[39mupper(), \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'multiprocessing' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T19:55:15.369233Z",
     "start_time": "2024-12-21T19:55:12.837847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# collaborative_filtering_optimized.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "import os\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Define Configuration Parameters\n",
    "# -----------------------------\n",
    "CONFIG_ALS = {\n",
    "    'interaction_filepath': '.venv/synthetic_profiles_with_reviews.csv',  # Path to interaction data\n",
    "    'als_model_filepath': '.venv/als_model.pth',  # Path to save the trained ALS model\n",
    "    'als_test_data_filepath': '.venv/als_test_data.csv',  # Path to save the test dataset\n",
    "    'strain_mapping_path': '.venv/mappings/strain_id_mapping.pkl',  # Path to strain mapping\n",
    "    'strain_embeddings_path': '.venv/strain_embeddings.npy',  # Path to strain embeddings\n",
    "    'logging': {\n",
    "        'level': 'INFO',  # Logging level\n",
    "        'file': '.venv/collaborative_filtering.log'  # Log file path\n",
    "    },\n",
    "    'training': {\n",
    "        'latent_factors': 50,  # Number of latent factors\n",
    "        'epochs': 100,  # Number of training epochs\n",
    "        'learning_rate': 0.005,  # Learning rate for the optimizer\n",
    "        # 'weight_decay': 1e-3,  # Weight decay (L2 regularization) -- Removed to fix the error\n",
    "        'batch_size': 512,  # Batch size for training\n",
    "        'patience': 10  # Early stopping patience\n",
    "    }\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Configure Logging\n",
    "# -----------------------------\n",
    "def configure_logging(log_level: str, log_file: str):\n",
    "    \"\"\"\n",
    "    Configure logging settings with rotation to manage log file size.\n",
    "    \"\"\"\n",
    "    numeric_level = getattr(logging, log_level.upper(), None)\n",
    "    if not isinstance(numeric_level, int):\n",
    "        numeric_level = logging.WARNING\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Prevent adding multiple handlers if they already exist\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(numeric_level)\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "        # Rotating file handler: 5MB per file, keep 5 backups\n",
    "        file_handler = RotatingFileHandler(log_file, maxBytes=5*1024*1024, backupCount=5)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = configure_logging(CONFIG_ALS['logging']['level'], CONFIG_ALS['logging']['file'])\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Define ALS Model with Bias Terms (PyTorch)\n",
    "# -----------------------------\n",
    "class ALSModel(nn.Module):\n",
    "    def __init__(self, num_users: int, num_strains: int, latent_factors: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize the ALS model with user and strain embeddings, including bias terms.\n",
    "        \"\"\"\n",
    "        super(ALSModel, self).__init__()\n",
    "        self.user_factors = nn.Embedding(num_users, latent_factors, sparse=True)  # Use sparse embeddings\n",
    "        self.strain_factors = nn.Embedding(num_strains, latent_factors, sparse=True)\n",
    "        self.user_bias = nn.Embedding(num_users, 1, sparse=True)\n",
    "        self.strain_bias = nn.Embedding(num_strains, 1, sparse=True)\n",
    "\n",
    "        # Initialize embeddings with a normal distribution\n",
    "        nn.init.normal_(self.user_factors.weight, 0, 0.1)\n",
    "        nn.init.normal_(self.strain_factors.weight, 0, 0.1)\n",
    "        nn.init.constant_(self.user_bias.weight, 0)\n",
    "        nn.init.constant_(self.strain_bias.weight, 0)\n",
    "\n",
    "    def forward(self, user_indices, strain_indices):\n",
    "        \"\"\"\n",
    "        Forward pass to compute the dot product of user and strain embeddings, including biases.\n",
    "        \"\"\"\n",
    "        user_embedding = self.user_factors(user_indices)\n",
    "        strain_embedding = self.strain_factors(strain_indices)\n",
    "        user_bias = self.user_bias(user_indices).squeeze()\n",
    "        strain_bias = self.strain_bias(strain_indices).squeeze()\n",
    "        return torch.sum(user_embedding * strain_embedding, dim=1) + user_bias + strain_bias\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Load and Preprocess Interaction Data\n",
    "# -----------------------------\n",
    "def load_interaction_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load user-item interaction data from a CSV file and preprocess it.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        logger.error(f\"Interaction data file does not exist at path: {filepath}\")\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "    try:\n",
    "        # Specify dtypes to optimize memory usage\n",
    "        dtype = {'user_id': 'int32', 'strain_id': 'int32', 'rating': 'float32'}\n",
    "        df = pd.read_csv(filepath, usecols=['user_id', 'strain_id', 'rating'], dtype=dtype)\n",
    "        expected_columns = {'user_id', 'strain_id', 'rating'}\n",
    "        if not expected_columns.issubset(df.columns):\n",
    "            missing = expected_columns - set(df.columns)\n",
    "            logger.error(f\"Interaction data is missing columns: {missing}\")\n",
    "            raise ValueError(f\"Missing columns in interaction data: {missing}\")\n",
    "        logger.info(f\"Loaded interaction data from {filepath} with shape {df.shape}\")\n",
    "    except pd.errors.ParserError:\n",
    "        logger.error(f\"Error parsing CSV file: {filepath}\")\n",
    "        raise\n",
    "\n",
    "    # Handle multiple ratings per user-strain pair by averaging\n",
    "    if df.duplicated(subset=['user_id', 'strain_id']).any():\n",
    "        logger.info(\"Found duplicate user-strain interactions. Aggregating by mean rating.\")\n",
    "        df = df.groupby(['user_id', 'strain_id'], as_index=False)['rating'].mean()\n",
    "        logger.info(f\"Aggregated interaction data to shape {df.shape}\")\n",
    "\n",
    "    # Additional checks for missing or anomalous data\n",
    "    if df['rating'].isnull().any():\n",
    "        logger.warning(\"Found missing ratings. Filling with the mean rating.\")\n",
    "        mean_rating = df['rating'].mean()\n",
    "        df['rating'].fillna(mean_rating, inplace=True)\n",
    "\n",
    "    # Ensure ratings are within the expected range (e.g., 1-5)\n",
    "    if df['rating'].min() < 1 or df['rating'].max() > 5:\n",
    "        logger.warning(\"Ratings found outside the range of 1-5. Clipping to the nearest valid value.\")\n",
    "        df['rating'] = df['rating'].clip(lower=1, upper=5)\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Train ALS Model\n",
    "# -----------------------------\n",
    "def train_als_model(interaction_df: pd.DataFrame, latent_factors: int = 10, epochs: int = 20, lr: float = 0.001,\n",
    "                   batch_size: int = 1024, patience: int = 10):\n",
    "    \"\"\"\n",
    "    Train the ALS model on user-item interaction data.\n",
    "    \"\"\"\n",
    "    logger.info(\"Preparing data for ALS model training...\")\n",
    "\n",
    "    # Convert user_id and strain_id to categorical codes with mapping\n",
    "    interaction_df['user_id'] = interaction_df['user_id'].astype('category')\n",
    "    interaction_df['strain_id'] = interaction_df['strain_id'].astype('category')\n",
    "\n",
    "    # Create mappings\n",
    "    user_categories = interaction_df['user_id'].cat.categories\n",
    "    strain_categories = interaction_df['strain_id'].cat.categories\n",
    "\n",
    "    user_mapping = {category: code for code, category in enumerate(user_categories)}\n",
    "    strain_mapping = {category: code for code, category in enumerate(strain_categories)}\n",
    "\n",
    "    # Encode IDs\n",
    "    interaction_df['user_id_code'] = interaction_df['user_id'].cat.codes\n",
    "    interaction_df['strain_id_code'] = interaction_df['strain_id'].cat.codes\n",
    "\n",
    "    num_users = len(user_categories)\n",
    "    num_strains = len(strain_categories)\n",
    "\n",
    "    logger.info(f\"Number of unique users: {num_users}, Number of unique strains: {num_strains}\")\n",
    "\n",
    "    # Save mappings for future use\n",
    "    os.makedirs('.venv/mappings', exist_ok=True)  # Create a directory for mappings\n",
    "    with open('.venv/mappings/user_id_mapping.pkl', 'wb') as f:\n",
    "        pickle.dump(user_mapping, f)\n",
    "    logger.info(\"Saved user_id mapping to '.venv/mappings/user_id_mapping.pkl'\")\n",
    "\n",
    "    with open('.venv/mappings/strain_id_mapping.pkl', 'wb') as f:\n",
    "        pickle.dump(strain_mapping, f)\n",
    "    logger.info(\"Saved strain_id mapping to '.venv/mappings/strain_id_mapping.pkl'\")\n",
    "\n",
    "    # Split data into training, validation, and testing sets (70-15-15 split)\n",
    "    train_df, temp_df = train_test_split(interaction_df, test_size=0.3, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    logger.info(f\"Training data size: {train_df.shape}, Validation data size: {val_df.shape}, Test data size: {test_df.shape}\")\n",
    "\n",
    "    # Convert data to PyTorch tensors (on CPU)\n",
    "    train_users = torch.tensor(train_df['user_id_code'].values, dtype=torch.long)\n",
    "    train_strains = torch.tensor(train_df['strain_id_code'].values, dtype=torch.long)\n",
    "    train_ratings = torch.tensor(train_df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    val_users = torch.tensor(val_df['user_id_code'].values, dtype=torch.long)\n",
    "    val_strains = torch.tensor(val_df['strain_id_code'].values, dtype=torch.long)\n",
    "    val_ratings = torch.tensor(val_df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    test_users = torch.tensor(test_df['user_id_code'].values, dtype=torch.long)\n",
    "    test_strains = torch.tensor(test_df['strain_id_code'].values, dtype=torch.long)\n",
    "    test_ratings = torch.tensor(test_df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader with batching\n",
    "    train_dataset = TensorDataset(train_users, train_strains, train_ratings)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    logger.info(\"Created DataLoader for training.\")\n",
    "\n",
    "    # Initialize ALS model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    model = ALSModel(num_users=num_users, num_strains=num_strains, latent_factors=latent_factors).to(device)\n",
    "    # Use SparseAdam optimizer for sparse embeddings without weight_decay\n",
    "    optimizer = optim.SparseAdam(model.parameters(), lr=lr)  # Removed weight_decay\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    best_rmse = float('inf')\n",
    "    best_model_state = copy.deepcopy(model.state_dict())\n",
    "    counter = 0  # Early stopping counter\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        model.train()  # Ensure model is in training mode\n",
    "\n",
    "        for batch_users, batch_strains, batch_ratings in train_loader:\n",
    "            # Move data to device\n",
    "            batch_users = batch_users.to(device)\n",
    "            batch_strains = batch_strains.to(device)\n",
    "            batch_ratings = batch_ratings.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_users, batch_strains)\n",
    "            loss = criterion(predictions, batch_ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * batch_users.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_dataset)\n",
    "        logger.info(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        rmse_val = evaluate_als_model(model, val_users, val_strains, val_ratings, criterion, device)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step(rmse_val)\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if rmse_val < best_rmse:\n",
    "            best_rmse = rmse_val\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "            logger.info(f\"New best RMSE: {best_rmse:.4f} at epoch {epoch + 1}\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            logger.info(f\"No improvement in RMSE for {counter} epochs.\")\n",
    "            if counter >= patience:\n",
    "                logger.info(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "    logger.info(f\"Best RMSE achieved: {best_rmse:.4f}\")\n",
    "\n",
    "    # Extract and save embeddings\n",
    "    extract_and_save_embeddings(model)\n",
    "\n",
    "    return model, test_df, test_users, test_strains, test_ratings\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Evaluate ALS Model\n",
    "# -----------------------------\n",
    "def evaluate_als_model(model, users, strains, ratings, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the ALS model on the validation set and compute RMSE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        users = users.to(device)\n",
    "        strains = strains.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "        predictions = model(users, strains)\n",
    "        predictions = torch.clamp(predictions, min=1, max=5)  # Ensure predictions are in valid range\n",
    "        mse = criterion(predictions, ratings).item()\n",
    "        rmse = np.sqrt(mse)\n",
    "        logger.info(f\"Validation RMSE: {rmse:.4f}\")\n",
    "        return rmse\n",
    "\n",
    "# -----------------------------\n",
    "# Step 7: Save ALS Model\n",
    "# -----------------------------\n",
    "def save_als_model(model, filepath: str):\n",
    "    \"\"\"\n",
    "    Save the trained ALS model's state dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        torch.save(model.state_dict(), filepath)\n",
    "        logger.info(f\"ALS model saved to '{filepath}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save ALS model to '{filepath}': {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 8: Save Test Data\n",
    "# -----------------------------\n",
    "def save_test_data(test_df: pd.DataFrame, filepath: str = '.venv/als_test_data.csv'):\n",
    "    \"\"\"\n",
    "    Save the test data used for evaluation to a CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load mappings\n",
    "        with open('.venv/mappings/user_id_mapping.pkl', 'rb') as f:\n",
    "            user_mapping = pickle.load(f)\n",
    "        with open('.venv/mappings/strain_id_mapping.pkl', 'rb') as f:\n",
    "            strain_mapping = pickle.load(f)\n",
    "\n",
    "        inverse_user_mapping = {v: k for k, v in user_mapping.items()}\n",
    "        inverse_strain_mapping = {v: k for k, v in strain_mapping.items()}\n",
    "\n",
    "        test_df_output = pd.DataFrame({\n",
    "            'user_id': test_df['user_id_code'].map(inverse_user_mapping),\n",
    "            'strain_id': test_df['strain_id_code'].map(inverse_strain_mapping),\n",
    "            'rating': test_df['rating'].values\n",
    "        })\n",
    "        test_df_output.to_csv(filepath, index=False)\n",
    "        logger.info(f\"Saved ALS test data to '{filepath}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save test data to '{filepath}': {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 9: Extract and Save Embeddings\n",
    "# -----------------------------\n",
    "def extract_and_save_embeddings(model):\n",
    "    \"\"\"\n",
    "    Extract user and strain embeddings from the trained model and save them as numpy arrays.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user_embeddings = model.user_factors.weight.detach().cpu().numpy()\n",
    "        strain_embeddings = model.strain_factors.weight.detach().cpu().numpy()\n",
    "\n",
    "        np.save('.venv/user_embeddings.npy', user_embeddings)\n",
    "        np.save('.venv/strain_embeddings.npy', strain_embeddings)\n",
    "        logger.info(\"User and strain embeddings saved to '.venv/user_embeddings.npy' and '.venv/strain_embeddings.npy'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract and save embeddings: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 10: Generate CF Predictions (Optional)\n",
    "# -----------------------------\n",
    "def generate_cf_predictions(model, num_users, num_strains):\n",
    "    \"\"\"\n",
    "    Generate CF predictions by computing the dot product of user and strain embeddings.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_factors = model.user_factors.weight.detach().cpu()\n",
    "        strain_factors = model.strain_factors.weight.detach().cpu()\n",
    "        cf_predictions = torch.matmul(user_factors, strain_factors.t()).numpy()\n",
    "    return cf_predictions\n",
    "\n",
    "# -----------------------------\n",
    "# Step 11: Main Execution for ALS\n",
    "# -----------------------------\n",
    "def main_als():\n",
    "    \"\"\"\n",
    "    Main function to execute the ALS training pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Load interaction data\n",
    "        interaction_df = load_interaction_data(CONFIG_ALS['interaction_filepath'])\n",
    "\n",
    "        # Step 2: Train ALS model\n",
    "        model_cf, test_df, test_users, test_strains, test_ratings = train_als_model(\n",
    "            interaction_df,\n",
    "            latent_factors=CONFIG_ALS['training']['latent_factors'],\n",
    "            epochs=CONFIG_ALS['training']['epochs'],\n",
    "            lr=CONFIG_ALS['training']['learning_rate'],\n",
    "            batch_size=CONFIG_ALS['training']['batch_size'],\n",
    "            patience=CONFIG_ALS['training']['patience']\n",
    "        )\n",
    "\n",
    "        # Step 3: Save ALS model\n",
    "        save_als_model(model_cf, CONFIG_ALS['als_model_filepath'])\n",
    "\n",
    "        # Step 4: Save ALS test data\n",
    "        save_test_data(test_df, CONFIG_ALS['als_test_data_filepath'])\n",
    "\n",
    "        # Optional Step 5: Generate CF predictions\n",
    "        # Uncomment if needed in the future\n",
    "        # cf_predictions = generate_cf_predictions(model_cf, len(user_mapping), len(strain_mapping))\n",
    "        # np.save('.venv/cf_predictions.npy', cf_predictions)\n",
    "        # logger.info(\"CF predictions saved to '.venv/cf_predictions.npy'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during ALS training: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_als()\n"
   ],
   "id": "653f7e9e4bee96be",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/c/Users/Ken/OneDrive/Desktop/Introduction to Statistical Learning/CapstoneV2/notebooks/.venv/collaborative_filtering.log'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 70\u001B[0m\n\u001B[1;32m     66\u001B[0m         logger\u001B[38;5;241m.\u001B[39maddHandler(console_handler)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logger\n\u001B[0;32m---> 70\u001B[0m logger \u001B[38;5;241m=\u001B[39m \u001B[43mconfigure_logging\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCONFIG_ALS\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlogging\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlevel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCONFIG_ALS\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlogging\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfile\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# -----------------------------\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# Step 3: Define ALS Model with Bias Terms (PyTorch)\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# -----------------------------\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mALSModel\u001B[39;00m(nn\u001B[38;5;241m.\u001B[39mModule):\n",
      "Cell \u001B[0;32mIn[4], line 59\u001B[0m, in \u001B[0;36mconfigure_logging\u001B[0;34m(log_level, log_file)\u001B[0m\n\u001B[1;32m     56\u001B[0m formatter \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mFormatter(\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%(asctime)s\u001B[39;00m\u001B[38;5;124m [\u001B[39m\u001B[38;5;132;01m%(levelname)s\u001B[39;00m\u001B[38;5;124m] \u001B[39m\u001B[38;5;132;01m%(message)s\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m# Rotating file handler: 5MB per file, keep 5 backups\u001B[39;00m\n\u001B[0;32m---> 59\u001B[0m file_handler \u001B[38;5;241m=\u001B[39m \u001B[43mRotatingFileHandler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaxBytes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackupCount\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m file_handler\u001B[38;5;241m.\u001B[39msetFormatter(formatter)\n\u001B[1;32m     61\u001B[0m logger\u001B[38;5;241m.\u001B[39maddHandler(file_handler)\n",
      "File \u001B[0;32m/usr/lib/python3.10/logging/handlers.py:155\u001B[0m, in \u001B[0;36mRotatingFileHandler.__init__\u001B[0;34m(self, filename, mode, maxBytes, backupCount, encoding, delay, errors)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m    154\u001B[0m     encoding \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mtext_encoding(encoding)\n\u001B[0;32m--> 155\u001B[0m \u001B[43mBaseRotatingHandler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mdelay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdelay\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaxBytes \u001B[38;5;241m=\u001B[39m maxBytes\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackupCount \u001B[38;5;241m=\u001B[39m backupCount\n",
      "File \u001B[0;32m/usr/lib/python3.10/logging/handlers.py:58\u001B[0m, in \u001B[0;36mBaseRotatingHandler.__init__\u001B[0;34m(self, filename, mode, encoding, delay, errors)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, filename, mode, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, delay\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     55\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    Use the specified filename for streamed logging\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m     \u001B[43mlogging\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFileHandler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdelay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m=\u001B[39m mode\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;241m=\u001B[39m encoding\n",
      "File \u001B[0;32m/usr/lib/python3.10/logging/__init__.py:1169\u001B[0m, in \u001B[0;36mFileHandler.__init__\u001B[0;34m(self, filename, mode, encoding, delay, errors)\u001B[0m\n\u001B[1;32m   1167\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1169\u001B[0m     StreamHandler\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/usr/lib/python3.10/logging/__init__.py:1201\u001B[0m, in \u001B[0;36mFileHandler._open\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1196\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1197\u001B[0m \u001B[38;5;124;03mOpen the current base file with the (original) mode and encoding.\u001B[39;00m\n\u001B[1;32m   1198\u001B[0m \u001B[38;5;124;03mReturn the resulting stream.\u001B[39;00m\n\u001B[1;32m   1199\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1200\u001B[0m open_func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_builtin_open\n\u001B[0;32m-> 1201\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopen_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbaseFilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1202\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/mnt/c/Users/Ken/OneDrive/Desktop/Introduction to Statistical Learning/CapstoneV2/notebooks/.venv/collaborative_filtering.log'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T19:18:14.444791Z",
     "start_time": "2024-10-18T18:17:35.101790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------\n",
    "# Deep Hybrid Recommender System\n",
    "# Comprehensive Professional-Grade Implementation\n",
    "# Refined for Enhanced Scalability and Performance with ROCm GPU Support\n",
    "# ------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, ndcg_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import shap  # SHAP for explainability\n",
    "import faiss  # Fast retrieval for real-time recommendations\n",
    "from typing import Tuple, Dict, Any, List, Set\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration Section\n",
    "# -----------------------------\n",
    "class Config:\n",
    "    # File paths\n",
    "    LOG_FILE = '.venv/deep_hybrid_recommender_overhauled.log'\n",
    "    USER_EMB_PATH = '.venv/user_embeddings.npy'\n",
    "    STRAIN_EMB_PATH = '.venv/strain_embeddings.npy'\n",
    "    STRAIN_DATA_PATH = '.venv/cleaned_strain_data_final_with_embeddings.csv'\n",
    "    INTERACTION_DATA_PATH = '.venv/synthetic_profiles_with_reviews.csv'\n",
    "    USER_MAPPING_PATH = '.venv/mappings/user_id_mapping.pkl'\n",
    "    STRAIN_MAPPING_PATH = '.venv/mappings/strain_id_mapping.pkl'\n",
    "    BEST_MODEL_PATH = '.venv/best_hybrid_model.pth'\n",
    "    SHAP_PLOT_PATH = '.venv/shap_summary_plot.png'\n",
    "    LOSS_PLOT_PATH = '.venv/loss_plot.png'\n",
    "    PRECISION_PLOT_PATH = '.venv/precision_plot.png'\n",
    "    RECALL_PLOT_PATH = '.venv/recall_plot.png'\n",
    "    PCA_USER_EMB_PATH = '.venv/pca_user_embeddings.pkl'\n",
    "    PCA_STRAIN_EMB_PATH = '.venv/pca_strain_embeddings.pkl'\n",
    "    PCA_CBF_EMB_PATH = '.venv/pca_cbf_embeddings.pkl'  # Updated path for CBF PCA\n",
    "\n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 256  # Consider increasing to 512 based on GPU memory availability\n",
    "    NUM_WORKERS = 8  # Increased to match CPU cores for better utilization\n",
    "    EPOCHS = 12\n",
    "    LEARNING_RATE = 0.0005\n",
    "    PATIENCE = 4\n",
    "    K = 10  # For Precision@K, Recall@K, NDCG@K\n",
    "\n",
    "    # PCA parameters (Desired components)\n",
    "    DESIRED_PCA_USER_COMPONENTS = 128\n",
    "    DESIRED_PCA_STRAIN_COMPONENTS = 128\n",
    "    DESIRED_PCA_CBF_COMPONENTS = 64\n",
    "\n",
    "    # Random seed for reproducibility\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "    # Faiss parameters\n",
    "    FAISS_INDEX = 'Flat'  # Options: 'Flat', 'IVFFlat', 'IVFPQ'\n",
    "\n",
    "# -----------------------------\n",
    "# Step 0: Configure Logging\n",
    "# -----------------------------\n",
    "def configure_logging(log_file_path: str) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Sets up logging to track the system's behavior.\n",
    "    Logs are written to both console and a log file with rotation to prevent unlimited growth.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger('DeepHybridRecommender')\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Prevent adding multiple handlers if the logger already has them\n",
    "    if not logger.handlers:\n",
    "        # Console handler\n",
    "        c_handler = logging.StreamHandler()\n",
    "        c_handler.setLevel(logging.INFO)\n",
    "\n",
    "        # File handler with rotation (max 5 files, each up to 5MB)\n",
    "        f_handler = logging.handlers.RotatingFileHandler(log_file_path, maxBytes=5*1024*1024, backupCount=5)\n",
    "        f_handler.setLevel(logging.INFO)\n",
    "\n",
    "        # Formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        c_handler.setFormatter(formatter)\n",
    "        f_handler.setFormatter(formatter)\n",
    "\n",
    "        # Add handlers\n",
    "        logger.addHandler(c_handler)\n",
    "        logger.addHandler(f_handler)\n",
    "\n",
    "    logger.info(\"Logging is configured.\")\n",
    "    return logger\n",
    "\n",
    "logger = configure_logging(Config.LOG_FILE)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load ALS Embeddings\n",
    "# -----------------------------\n",
    "def load_embeddings(user_emb_path: str, strain_emb_path: str, logger: logging.Logger) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load ALS embeddings from specified file paths.\n",
    "    Handles missing files and NaN values.\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading ALS embeddings...\")\n",
    "    for path in [user_emb_path, strain_emb_path]:\n",
    "        if not os.path.exists(path):\n",
    "            logger.error(f\"Embedding file not found at path: {path}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        user_embeddings = np.load(user_emb_path, mmap_mode='r')\n",
    "        strain_embeddings = np.load(strain_emb_path, mmap_mode='r')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading embeddings: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Log embeddings' shapes\n",
    "    logger.info(f\"User Embeddings Shape: {user_embeddings.shape}\")\n",
    "    logger.info(f\"Strain Embeddings Shape: {strain_embeddings.shape}\")\n",
    "\n",
    "    # Handle NaNs\n",
    "    if np.isnan(user_embeddings).any() or np.isnan(strain_embeddings).any():\n",
    "        logger.warning(\"Found NaN values in ALS embeddings. Replacing NaNs with zeros.\")\n",
    "        user_embeddings = np.nan_to_num(user_embeddings)\n",
    "        strain_embeddings = np.nan_to_num(strain_embeddings)\n",
    "\n",
    "    logger.info(f\"Loaded user embeddings shape after NaN handling: {user_embeddings.shape}\")\n",
    "    logger.info(f\"Loaded strain embeddings shape after NaN handling: {strain_embeddings.shape}\")\n",
    "    return user_embeddings, strain_embeddings\n",
    "\n",
    "user_embeddings, strain_embeddings = load_embeddings(\n",
    "    Config.USER_EMB_PATH, \n",
    "    Config.STRAIN_EMB_PATH, \n",
    "    logger\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Load Content-based Data\n",
    "# -----------------------------\n",
    "def load_strain_data(strain_data_path: str, logger: logging.Logger) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load content-based strain data and extract embeddings.\n",
    "    Handles missing files and NaN values.\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading content-based strain data...\")\n",
    "    if not os.path.exists(strain_data_path):\n",
    "        logger.error(f\"Strain data file not found at path: {strain_data_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        strain_data = pd.read_csv(strain_data_path)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading strain data: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Handle NaNs in strain data\n",
    "    if strain_data.isnull().values.any():\n",
    "        logger.warning(\"Found NaN values in strain data. Replacing NaNs with zeros.\")\n",
    "        strain_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Extract embedding columns dynamically\n",
    "    embedding_columns = [col for col in strain_data.columns if col.startswith('embedding_')]\n",
    "    if not embedding_columns:\n",
    "        logger.error(\"No embedding columns found in strain data.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    strain_embeddings_cbf = strain_data[embedding_columns].values\n",
    "\n",
    "    # Handle NaNs in embeddings\n",
    "    if np.isnan(strain_embeddings_cbf).any():\n",
    "        logger.warning(\"Found NaN values in content-based embeddings. Replacing NaNs with zeros.\")\n",
    "        strain_embeddings_cbf = np.nan_to_num(strain_embeddings_cbf)\n",
    "\n",
    "    logger.info(f\"Extracted content-based embeddings shape: {strain_embeddings_cbf.shape}\")\n",
    "    return strain_data, strain_embeddings_cbf\n",
    "\n",
    "strain_data, strain_embeddings_cbf = load_strain_data(Config.STRAIN_DATA_PATH, logger)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Load Interaction Data (Ratings)\n",
    "# -----------------------------\n",
    "def load_interaction_data(interaction_data_path: str, logger: logging.Logger) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load user interaction data, such as ratings, and clean it.\n",
    "    Handles missing files and NaN values.\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading interaction data...\")\n",
    "    if not os.path.exists(interaction_data_path):\n",
    "        logger.error(f\"Interaction data file not found at path: {interaction_data_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        interaction_data = pd.read_csv(interaction_data_path, usecols=['user_id', 'strain_id', 'rating'])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading interaction data: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    initial_shape = interaction_data.shape\n",
    "    interaction_data.dropna(inplace=True)  # Ensure no missing ratings\n",
    "    if interaction_data.shape != initial_shape:\n",
    "        logger.warning(f\"Dropped {initial_shape[0] - interaction_data.shape[0]} rows due to NaNs in interaction data.\")\n",
    "\n",
    "    # Filter out invalid ratings if necessary\n",
    "    valid_rating_mask = (interaction_data['rating'] >= 1) & (interaction_data['rating'] <= 5)\n",
    "    if not valid_rating_mask.all():\n",
    "        num_invalid = (~valid_rating_mask).sum()\n",
    "        logger.warning(f\"Found {num_invalid} invalid ratings. Dropping these rows.\")\n",
    "        interaction_data = interaction_data[valid_rating_mask]\n",
    "\n",
    "    logger.info(f\"Loaded interaction data shape: {interaction_data.shape}\")\n",
    "    return interaction_data\n",
    "\n",
    "interaction_data = load_interaction_data(Config.INTERACTION_DATA_PATH, logger)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Load and Verify Mappings\n",
    "# -----------------------------\n",
    "def load_mappings(user_mapping_path: str, strain_mapping_path: str, logger: logging.Logger) -> Tuple[Dict[Any, int], Dict[Any, int]]:\n",
    "    \"\"\"\n",
    "    Load user and strain mappings from pickle files.\n",
    "    Handles missing files and loading errors.\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading user and strain mappings...\")\n",
    "    for path in [user_mapping_path, strain_mapping_path]:\n",
    "        if not os.path.exists(path):\n",
    "            logger.error(f\"Mapping file not found at path: {path}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        with open(user_mapping_path, 'rb') as f:\n",
    "            user_mapping = pickle.load(f)\n",
    "        with open(strain_mapping_path, 'rb') as f:\n",
    "            strain_mapping = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading mappings: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    logger.info(f\"Loaded user_mapping with {len(user_mapping)} entries.\")\n",
    "    logger.info(f\"Loaded strain_mapping with {len(strain_mapping)} entries.\")\n",
    "    return user_mapping, strain_mapping\n",
    "\n",
    "user_mapping, strain_mapping = load_mappings(\n",
    "    Config.USER_MAPPING_PATH, \n",
    "    Config.STRAIN_MAPPING_PATH, \n",
    "    logger\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Map IDs and Ensure Data Integrity\n",
    "# -----------------------------\n",
    "def map_ids_and_verify(interaction_df: pd.DataFrame, \n",
    "                       user_mapping: Dict[Any, int], \n",
    "                       strain_mapping: Dict[Any, int], \n",
    "                       logger: logging.Logger) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map user and strain IDs to numerical codes and ensure data integrity.\n",
    "    Drops rows with missing mappings.\n",
    "    \"\"\"\n",
    "    logger.info(\"Mapping user_id and strain_id to codes...\")\n",
    "    interaction_df['user_id_code'] = interaction_df['user_id'].map(user_mapping)\n",
    "    interaction_df['strain_id_code'] = interaction_df['strain_id'].map(strain_mapping)\n",
    "\n",
    "    # Drop rows with missing mappings\n",
    "    num_missing_users = interaction_df['user_id_code'].isnull().sum()\n",
    "    num_missing_strains = interaction_df['strain_id_code'].isnull().sum()\n",
    "    if num_missing_users > 0 or num_missing_strains > 0:\n",
    "        logger.warning(f\"Missing mappings: {num_missing_users} users, {num_missing_strains} strains\")\n",
    "        interaction_df.dropna(subset=['user_id_code', 'strain_id_code'], inplace=True)\n",
    "        logger.info(f\"Dropped rows with missing mappings. New shape: {interaction_df.shape}\")\n",
    "\n",
    "    # Ensure no NaNs after mapping\n",
    "    if interaction_df[['user_id_code', 'strain_id_code']].isnull().any().any():\n",
    "        logger.error(\"Found NaN values after mapping IDs. Please check your data.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Convert to integer type\n",
    "    interaction_df['user_id_code'] = interaction_df['user_id_code'].astype(int)\n",
    "    interaction_df['strain_id_code'] = interaction_df['strain_id_code'].astype(int)\n",
    "    logger.info(\"Completed mapping of IDs.\")\n",
    "    return interaction_df\n",
    "\n",
    "interaction_data = map_ids_and_verify(\n",
    "    interaction_data, \n",
    "    user_mapping, \n",
    "    strain_mapping, \n",
    "    logger\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Split Data into Train and Validation Sets\n",
    "# -----------------------------\n",
    "def split_data(interaction_df: pd.DataFrame, \n",
    "              test_size: float = 0.2, \n",
    "              random_state: int = 42, \n",
    "              logger: logging.Logger = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split interaction data into training and validation sets.\n",
    "    \"\"\"\n",
    "    if logger:\n",
    "        logger.info(\"Splitting data into training and validation sets...\")\n",
    "    try:\n",
    "        train_df, val_df = train_test_split(\n",
    "            interaction_df, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Error during train-test split: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if logger:\n",
    "        logger.info(f\"Training data size: {train_df.shape[0]}, Validation data size: {val_df.shape[0]}\")\n",
    "\n",
    "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "\n",
    "train_df, val_df = split_data(\n",
    "    interaction_data, \n",
    "    test_size=0.2, \n",
    "    random_state=Config.RANDOM_STATE, \n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 7: Apply PCA for Dimensionality Reduction\n",
    "# -----------------------------\n",
    "def apply_pca(embeddings: np.ndarray, desired_components: int, logger: logging.Logger, save_path: str) -> Tuple[np.ndarray, PCA]:\n",
    "    \"\"\"\n",
    "    Applies PCA to reduce the dimensionality of embeddings.\n",
    "    Dynamically sets n_components to min(desired_components, min(n_samples, n_features)).\n",
    "    \"\"\"\n",
    "    n_samples, n_features = embeddings.shape\n",
    "    actual_components = min(desired_components, min(n_samples, n_features))\n",
    "    logger.info(f\"Applying PCA: desired n_components={desired_components}, actual n_components={actual_components}\")\n",
    "    pca = PCA(n_components=actual_components, random_state=Config.RANDOM_STATE)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "    logger.info(f\"PCA completed. Reduced embeddings shape: {reduced_embeddings.shape}, Explained Variance: {explained_variance:.4f}\")\n",
    "\n",
    "    # Save PCA object for future transformations\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(pca, f)\n",
    "    logger.info(f\"PCA model saved at {save_path}.\")\n",
    "\n",
    "    return reduced_embeddings, pca\n",
    "\n",
    "def load_pca(pca_path: str, logger: logging.Logger) -> PCA:\n",
    "    \"\"\"\n",
    "    Loads a saved PCA model.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pca_path):\n",
    "        logger.error(f\"PCA file not found at path: {pca_path}\")\n",
    "        sys.exit(1)\n",
    "    with open(pca_path, 'rb') as f:\n",
    "        pca = pickle.load(f)\n",
    "    logger.info(f\"PCA model loaded from {pca_path}.\")\n",
    "    return pca\n",
    "\n",
    "def apply_pca_cbf(embeddings_cbf: np.ndarray, desired_components: int, logger: logging.Logger, save_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies PCA to reduce the dimensionality of content-based embeddings.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = embeddings_cbf.shape\n",
    "    actual_components = min(desired_components, min(n_samples, n_features))\n",
    "    logger.info(f\"Applying PCA to content-based embeddings: desired n_components={desired_components}, actual n_components={actual_components}\")\n",
    "    pca_cbf = PCA(n_components=actual_components, random_state=Config.RANDOM_STATE)\n",
    "    strain_embeddings_cbf_reduced = pca_cbf.fit_transform(embeddings_cbf)\n",
    "    explained_variance = np.sum(pca_cbf.explained_variance_ratio_)\n",
    "    logger.info(f\"PCA applied to content-based embeddings. Reduced shape: {strain_embeddings_cbf_reduced.shape}, Explained Variance: {explained_variance:.4f}\")\n",
    "\n",
    "    # Save PCA for CBF embeddings\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(pca_cbf, f)\n",
    "    logger.info(f\"PCA model for CBF embeddings saved at {save_path}.\")\n",
    "\n",
    "    return strain_embeddings_cbf_reduced\n",
    "\n",
    "# Apply PCA to ALS embeddings\n",
    "user_embeddings_reduced, pca_user = apply_pca(\n",
    "    user_embeddings, \n",
    "    Config.DESIRED_PCA_USER_COMPONENTS, \n",
    "    logger, \n",
    "    Config.PCA_USER_EMB_PATH\n",
    ")\n",
    "\n",
    "strain_embeddings_reduced, pca_strain = apply_pca(\n",
    "    strain_embeddings, \n",
    "    Config.DESIRED_PCA_STRAIN_COMPONENTS, \n",
    "    logger, \n",
    "    Config.PCA_STRAIN_EMB_PATH\n",
    ")\n",
    "\n",
    "# Apply PCA to content-based embeddings\n",
    "strain_embeddings_cbf_reduced = apply_pca_cbf(\n",
    "    strain_embeddings_cbf, \n",
    "    Config.DESIRED_PCA_CBF_COMPONENTS, \n",
    "    logger, \n",
    "    Config.PCA_CBF_EMB_PATH  # Updated to use the correct path\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 8: Create Datasets\n",
    "# -----------------------------\n",
    "class HybridFeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for hybrid feature integration.\n",
    "    Concatenates user ALS embeddings, strain ALS embeddings, and strain CBF embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 interaction_df: pd.DataFrame, \n",
    "                 user_embeddings: np.ndarray, \n",
    "                 strain_embeddings: np.ndarray, \n",
    "                 strain_embeddings_cbf: np.ndarray, \n",
    "                 logger: logging.Logger):\n",
    "        self.user_ids = interaction_df['user_id_code'].values\n",
    "        self.strain_ids = interaction_df['strain_id_code'].values\n",
    "        self.ratings = interaction_df['rating'].values.astype(np.float32)\n",
    "        self.user_embeddings = user_embeddings\n",
    "        self.strain_embeddings = strain_embeddings\n",
    "        self.strain_embeddings_cbf = strain_embeddings_cbf\n",
    "        self.logger = logger\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, float, int, int]:\n",
    "        user_id = self.user_ids[idx]\n",
    "        strain_id = self.strain_ids[idx]\n",
    "        \n",
    "        try:\n",
    "            user_emb = self.user_embeddings[user_id]\n",
    "            strain_emb = self.strain_embeddings[strain_id]\n",
    "            strain_emb_cbf = self.strain_embeddings_cbf[strain_id]\n",
    "        except IndexError as e:\n",
    "            self.logger.error(f\"IndexError at idx {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Handle NaNs\n",
    "        if np.isnan(user_emb).any() or np.isnan(strain_emb).any() or np.isnan(strain_emb_cbf).any():\n",
    "            self.logger.warning(f\"NaN found in features at index {idx}. Replacing NaNs with zeros.\")\n",
    "            user_emb = np.nan_to_num(user_emb)\n",
    "            strain_emb = np.nan_to_num(strain_emb)\n",
    "            strain_emb_cbf = np.nan_to_num(strain_emb_cbf)\n",
    "        \n",
    "        # Concatenate features\n",
    "        hybrid_feature = np.concatenate([user_emb, strain_emb, strain_emb_cbf]).astype(np.float32)\n",
    "        rating = self.ratings[idx]\n",
    "        \n",
    "        return hybrid_feature, rating, user_id, strain_id\n",
    "\n",
    "def create_datasets(train_df: pd.DataFrame, \n",
    "                    val_df: pd.DataFrame, \n",
    "                    user_embeddings: np.ndarray, \n",
    "                    strain_embeddings: np.ndarray, \n",
    "                    strain_embeddings_cbf: np.ndarray, \n",
    "                    logger: logging.Logger) -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Create training and validation datasets.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating training and validation datasets...\")\n",
    "    try:\n",
    "        train_dataset = HybridFeatureDataset(\n",
    "            interaction_df=train_df, \n",
    "            user_embeddings=user_embeddings, \n",
    "            strain_embeddings=strain_embeddings, \n",
    "            strain_embeddings_cbf=strain_embeddings_cbf, \n",
    "            logger=logger\n",
    "        )\n",
    "        val_dataset = HybridFeatureDataset(\n",
    "            interaction_df=val_df, \n",
    "            user_embeddings=user_embeddings, \n",
    "            strain_embeddings=strain_embeddings, \n",
    "            strain_embeddings_cbf=strain_embeddings_cbf, \n",
    "            logger=logger\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating datasets: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    logger.info(\"Datasets created successfully.\")\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "train_dataset, val_dataset = create_datasets(\n",
    "    train_df, \n",
    "    val_df, \n",
    "    user_embeddings_reduced, \n",
    "    strain_embeddings_reduced, \n",
    "    strain_embeddings_cbf_reduced, \n",
    "    logger\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 9: Create DataLoaders\n",
    "# -----------------------------\n",
    "def create_dataloaders(train_dataset: Dataset, \n",
    "                       val_dataset: Dataset, \n",
    "                       batch_size: int, \n",
    "                       num_workers: int, \n",
    "                       logger: logging.Logger) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create DataLoaders for training and validation datasets.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating DataLoaders...\")\n",
    "    try:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True  # Enabled pin_memory for GPU\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True  # Enabled pin_memory for GPU\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating DataLoaders: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    logger.info(\"DataLoaders created successfully.\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    num_workers=Config.NUM_WORKERS, \n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 10: Define Deep Hybrid Model\n",
    "# -----------------------------\n",
    "class DeepHybridRecommender(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network architecture for hybrid recommendations.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int):\n",
    "        super(DeepHybridRecommender, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 11: Enhanced Training and Evaluation\n",
    "# -----------------------------\n",
    "def precision_recall_at_k_per_user(y_true: Dict[int, Set[int]], \n",
    "                                   y_pred: Dict[int, List[int]], \n",
    "                                   k: int) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate Precision@k and Recall@k per user and return the average.\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for user, pred_items in y_pred.items():\n",
    "        true_items = y_true.get(user, set())\n",
    "        pred_items = pred_items[:k]\n",
    "        \n",
    "        if not true_items:\n",
    "            continue  # Skip users with no relevant items\n",
    "        \n",
    "        relevant_pred = len(set(pred_items) & true_items)\n",
    "        precision = relevant_pred / k\n",
    "        recall = relevant_pred / len(true_items)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    \n",
    "    return avg_precision, avg_recall\n",
    "\n",
    "def evaluate_model(model: nn.Module, \n",
    "                   loader: DataLoader, \n",
    "                   device: torch.device, \n",
    "                   k: int = 10) -> Tuple[float, float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given data loader and return evaluation metrics.\n",
    "    Computes RMSE, MAE, NDCG@k, Precision@k, and Recall@k.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    user_true, user_pred = {}, {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, ratings, user_ids, strain_ids in tqdm(loader, desc=\"Evaluating\"):\n",
    "            features = features.to(device)\n",
    "            predictions = model(features).cpu().numpy().flatten()\n",
    "            ratings = ratings.cpu().numpy().flatten()\n",
    "            user_ids = user_ids.numpy().flatten()\n",
    "            strain_ids = strain_ids.numpy().flatten()\n",
    "            \n",
    "            y_true.extend(ratings)\n",
    "            y_pred.extend(predictions)\n",
    "            \n",
    "            for user, strain, rating, pred in zip(user_ids, strain_ids, ratings, predictions):\n",
    "                if user not in user_true:\n",
    "                    user_true[user] = set()\n",
    "                if rating > 3:  # Define relevant items (threshold can be adjusted)\n",
    "                    user_true[user].add(strain)\n",
    "                \n",
    "                if user not in user_pred:\n",
    "                    user_pred[user] = []\n",
    "                user_pred[user].append((strain, pred))\n",
    "    \n",
    "    # Compute RMSE and MAE on continuous predictions\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Compute NDCG@k\n",
    "    # For NDCG, we need relevance scores. Assuming binary relevance here.\n",
    "    # This might not be the most accurate, but aligns with the thresholding.\n",
    "    if len(y_true) > 0:\n",
    "        # Reshape y_true and y_pred for ndcg_score\n",
    "        # Note: ndcg_score expects 2D arrays\n",
    "        y_true_matrix = np.array([[1 if rating > 3 else 0 for rating in y_true]])\n",
    "        y_pred_matrix = np.array([y_pred])\n",
    "        ndcg = ndcg_score(y_true_matrix, y_pred_matrix, k=k)\n",
    "    else:\n",
    "        ndcg = 0\n",
    "    \n",
    "    # For ranking metrics\n",
    "    # For each user, sort the predictions and get top-k\n",
    "    for user in user_pred:\n",
    "        sorted_strains = sorted(user_pred[user], key=lambda x: x[1], reverse=True)\n",
    "        # Ensure exactly k items by padding with dummy items\n",
    "        if len(sorted_strains) < k:\n",
    "            sorted_strains += [(0, 0.0)] * (k - len(sorted_strains))\n",
    "        user_pred[user] = [item for item, score in sorted_strains[:k]]\n",
    "    \n",
    "    # Calculate per-user precision and recall\n",
    "    precision, recall = precision_recall_at_k_per_user(user_true, user_pred, k)\n",
    "    \n",
    "    return mse, rmse, mae, ndcg, precision, recall\n",
    "\n",
    "def train_model(train_loader: DataLoader, \n",
    "                val_loader: DataLoader, \n",
    "                input_size: int, \n",
    "                epochs: int = 12, \n",
    "                lr: float = 0.0005, \n",
    "                k: int = 10, \n",
    "                patience: int = 4) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Train the hybrid recommender model.\n",
    "    Implements early stopping and learning rate scheduling.\n",
    "    \"\"\"\n",
    "    # Detect ROCm-supported GPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')  # ROCm uses 'cuda' as device identifier\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    model = DeepHybridRecommender(input_size=input_size).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = Config.BEST_MODEL_PATH\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # Initialize histories\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    precision_history = []\n",
    "    recall_history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        logger.info(f\"Starting Epoch {epoch}/{epochs}\")\n",
    "        for batch_idx, (features, ratings, _, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")):\n",
    "            features = features.to(device)\n",
    "            ratings = ratings.to(device).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(features)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        logger.info(f\"Epoch {epoch} Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        mse, rmse, mae, ndcg, precision, recall = evaluate_model(model, val_loader, device, k=k)\n",
    "        val_losses.append(rmse)\n",
    "        precision_history.append(precision)\n",
    "        recall_history.append(recall)\n",
    "        \n",
    "        logger.info(f\"Epoch {epoch} Validation RMSE: {rmse:.4f}, MAE: {mae:.4f}, NDCG@{k}: {ndcg:.4f}, \"\n",
    "                    f\"Precision@{k}: {precision:.4f}, Recall@{k}: {recall:.4f}\")\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step(rmse)\n",
    "        \n",
    "        # Check for improvement\n",
    "        if rmse < best_val_loss:\n",
    "            best_val_loss = rmse\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            logger.info(f\"Epoch {epoch}: New best model saved.\")\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            logger.info(f\"Epoch {epoch}: No improvement in validation RMSE.\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            logger.info(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "            break\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plot_loss(train_losses, val_losses)\n",
    "    # Plot precision and recall over epochs\n",
    "    plot_metric_over_epochs(precision_history, \"Precision@10\")\n",
    "    plot_metric_over_epochs(recall_history, \"Recall@10\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plot_loss(train_losses: List[float], val_losses: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss over epochs.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation RMSE')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss / RMSE\")\n",
    "    plt.title(\"Training Loss and Validation RMSE\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(Config.LOSS_PLOT_PATH)\n",
    "    plt.close()\n",
    "    logger.info(f\"Loss plot saved as '{Config.LOSS_PLOT_PATH}'.\")\n",
    "\n",
    "def plot_metric_over_epochs(metrics: List[float], metric_name: str):\n",
    "    \"\"\"\n",
    "    Plot a given metric over epochs.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(metrics, label=metric_name)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f\"{metric_name} Over Epochs\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    metric_plot_path = f'{metric_name.replace(\"@\", \"\").replace(\" \", \"_\").lower()}_plot.png'\n",
    "    plt.savefig(os.path.join('.venv', metric_plot_path))\n",
    "    plt.close()\n",
    "    logger.info(f\"{metric_name} plot saved as '{os.path.join('.venv', metric_plot_path)}'.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 12: Compute SHAP Values\n",
    "# -----------------------------\n",
    "def compute_shap_values(model: nn.Module, \n",
    "                        sample_features: torch.Tensor, \n",
    "                        logger: logging.Logger, \n",
    "                        config: Config):\n",
    "    \"\"\"\n",
    "    Compute SHAP values for the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Computing SHAP values...\")\n",
    "        model_cpu = DeepHybridRecommender(input_size=sample_features.shape[1]).to('cpu')\n",
    "        model_cpu.load_state_dict(model.state_dict())\n",
    "        model_cpu.eval()\n",
    "        \n",
    "        sample_features_cpu = sample_features.to('cpu')  # Ensure features are on CPU\n",
    "        \n",
    "        # Initialize SHAP DeepExplainer\n",
    "        explainer = shap.DeepExplainer(model_cpu, sample_features_cpu)\n",
    "        shap_values = explainer.shap_values(sample_features_cpu)\n",
    "        \n",
    "        # Generate SHAP summary plot\n",
    "        shap.summary_plot(shap_values, sample_features_cpu.numpy(), show=False)\n",
    "        plt.savefig(config.SHAP_PLOT_PATH)\n",
    "        plt.close()\n",
    "        logger.info(f\"SHAP summary plot saved as '{config.SHAP_PLOT_PATH}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"SHAP explainability failed: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 13: Real-Time Recommendation with Faiss\n",
    "# -----------------------------\n",
    "def build_faiss_index(strain_embeddings: np.ndarray, \n",
    "                      config: Config, \n",
    "                      logger: logging.Logger) -> faiss.Index:\n",
    "    \"\"\"\n",
    "    Build a Faiss index for strain embeddings using specified index type.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        index_type = config.FAISS_INDEX\n",
    "        d = strain_embeddings.shape[1]\n",
    "        logger.info(f\"Building Faiss index of type {index_type} with dimension {d}.\")\n",
    "        \n",
    "        if index_type == 'Flat':\n",
    "            index = faiss.IndexFlatL2(d)\n",
    "        elif index_type == 'IVFFlat':\n",
    "            nlist = 100  # Number of clusters, can be tuned\n",
    "            quantizer = faiss.IndexFlatL2(d)\n",
    "            index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)\n",
    "            # Train the index\n",
    "            logger.info(\"Training Faiss index...\")\n",
    "            index.train(strain_embeddings.astype(np.float32))\n",
    "        elif index_type == 'IVFPQ':\n",
    "            nlist = 100\n",
    "            m = 16  # Number of subquantizers\n",
    "            nbits = 8  # Number of bits per subquantizer\n",
    "            quantizer = faiss.IndexFlatL2(d)\n",
    "            index = faiss.IndexIVFPQ(quantizer, d, nlist, m, nbits)\n",
    "            # Train the index\n",
    "            logger.info(\"Training Faiss index...\")\n",
    "            index.train(strain_embeddings.astype(np.float32))\n",
    "        else:\n",
    "            logger.error(f\"Unsupported Faiss index type: {index_type}\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        # Add vectors to the index\n",
    "        index.add(strain_embeddings.astype(np.float32))\n",
    "        logger.info(f\"Faiss index built with {index.ntotal} vectors.\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building Faiss index: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def real_time_recommendation(query_embedding: np.ndarray, \n",
    "                             faiss_index: faiss.Index, \n",
    "                             strain_data: pd.DataFrame, \n",
    "                             top_k: int = 10, \n",
    "                             logger: logging.Logger = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k similar strains using Faiss and return strain names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        D, I = faiss_index.search(query_embedding.astype(np.float32), top_k)\n",
    "        \n",
    "        # Convert indices to strain names\n",
    "        recommended_strains = []\n",
    "        for idx in I[0]:\n",
    "            if idx < len(strain_data):\n",
    "                strain_name = strain_data.iloc[idx]['Strain_Name']\n",
    "                recommended_strains.append(strain_name)\n",
    "            else:\n",
    "                recommended_strains.append(\"Unknown Strain\")\n",
    "        \n",
    "        if logger:\n",
    "            logger.info(f\"Recommended strains: {recommended_strains}\")\n",
    "        \n",
    "        return recommended_strains\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Error during real-time recommendation: {e}\")\n",
    "        return []\n",
    "\n",
    "# -----------------------------\n",
    "# Step 14: Diversity and Serendipity Evaluation\n",
    "# -----------------------------\n",
    "def calculate_diversity(recommendations: List[str], \n",
    "                        strain_embeddings: np.ndarray, \n",
    "                        strain_data: pd.DataFrame, \n",
    "                        logger: logging.Logger) -> float:\n",
    "    \"\"\"\n",
    "    Measure diversity of recommendations using pairwise cosine similarity.\n",
    "    Diversity is defined as 1 - average cosine similarity between recommended items.\n",
    "    \"\"\"\n",
    "    if len(recommendations) < 2:\n",
    "        logger.warning(\"Not enough recommendations to calculate diversity.\")\n",
    "        return 0.0  # No diversity if less than 2 items\n",
    "    \n",
    "    # Convert strain names back to indices\n",
    "    recommended_indices = []\n",
    "    for name in recommendations:\n",
    "        idx = strain_data[strain_data['Strain_Name'] == name].index\n",
    "        if not idx.empty:\n",
    "            recommended_indices.append(idx[0])\n",
    "        else:\n",
    "            logger.warning(f\"Strain name '{name}' not found in strain_data.\")\n",
    "    \n",
    "    if len(recommended_indices) < 2:\n",
    "        logger.warning(\"Not enough valid recommendations to calculate diversity.\")\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        pairwise_sim = cosine_similarity(strain_embeddings[recommended_indices])\n",
    "        triu_indices = np.triu_indices_from(pairwise_sim, k=1)\n",
    "        avg_cosine_similarity = np.mean(pairwise_sim[triu_indices])\n",
    "        avg_diversity = 1 - avg_cosine_similarity  # Ensure diversity does not exceed 1\n",
    "        # Clamp the value between 0 and 1\n",
    "        avg_diversity = np.clip(avg_diversity, 0, 1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating diversity: {e}\")\n",
    "        return 0.0\n",
    "    \n",
    "    return avg_diversity\n",
    "\n",
    "def calculate_serendipity(recommendations: List[str], \n",
    "                          true_likes: Set[int], \n",
    "                          strain_data: pd.DataFrame, \n",
    "                          logger: logging.Logger) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate serendipity by checking how often unexpected items are relevant.\n",
    "    Serendipity is defined as the proportion of recommendations that are unexpected.\n",
    "    \"\"\"\n",
    "    # Define 'unexpected' as items not interacted with by the user\n",
    "    # Assuming true_likes are the items the user has interacted with (rating > 3)\n",
    "    # Here, 'unexpected but relevant' could mean items that the user hasn't interacted with but are actually relevant\n",
    "    # However, in this context, relevance is defined as rating >3, which would only include true_likes\n",
    "    # Thus, serendipity might require a different definition or additional data\n",
    "    # For demonstration, we'll consider it as the proportion of recommendations that are unexpected\n",
    "    \n",
    "    # Get all recommended strain codes\n",
    "    recommended_strain_codes = []\n",
    "    for name in recommendations:\n",
    "        # Assuming 'strain_id' exists in strain_data\n",
    "        strain_ids = strain_data[strain_data['Strain_Name'] == name]['strain_id'].values\n",
    "        if len(strain_ids) > 0:\n",
    "            strain_code = strain_mapping.get(strain_ids[0], None)\n",
    "            if strain_code is not None:\n",
    "                recommended_strain_codes.append(strain_code)\n",
    "            else:\n",
    "                logger.warning(f\"Strain code for strain_id {strain_ids[0]} not found in strain_mapping.\")\n",
    "        else:\n",
    "            logger.warning(f\"Strain name '{name}' not found in strain_data.\")\n",
    "    \n",
    "    unexpected_recommended = set(recommended_strain_codes) - true_likes\n",
    "    # Define serendipity as the proportion of recommendations that are unexpected\n",
    "    serendipity = len(unexpected_recommended) / len(recommendations) if recommendations else 0\n",
    "    return serendipity\n",
    "\n",
    "# -----------------------------\n",
    "# Step 15: Main Execution\n",
    "# -----------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to train and evaluate the model.\n",
    "    Also handles visualization and recommendation evaluation.\n",
    "    \"\"\"\n",
    "    # Ensure input size matches the concatenated embeddings\n",
    "    input_size = user_embeddings_reduced.shape[1] + strain_embeddings_reduced.shape[1] + strain_embeddings_cbf_reduced.shape[1]\n",
    "    logger.info(f\"Input size for the model: {input_size}\")\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(train_loader, val_loader, input_size)\n",
    "\n",
    "    # Load the best model\n",
    "    if os.path.exists(Config.BEST_MODEL_PATH):\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        model.load_state_dict(torch.load(Config.BEST_MODEL_PATH, map_location=device))\n",
    "        logger.info(\"Loaded the best model from disk.\")\n",
    "    else:\n",
    "        logger.warning(\"Best model path not found. Using the current model.\")\n",
    "\n",
    "    # SHAP for Explainability\n",
    "    try:\n",
    "        logger.info(\"Computing SHAP values for model explainability...\")\n",
    "        model.eval()\n",
    "        # Select a subset of the training data for SHAP\n",
    "        sample_features, _, _, _ = next(iter(train_loader))\n",
    "        sample_features = sample_features[:10].to('cpu')  # Ensure on CPU for SHAP compatibility\n",
    "        compute_shap_values(model, sample_features, logger, Config)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"SHAP explainability failed: {e}\")\n",
    "\n",
    "    # Build Faiss Index\n",
    "    try:\n",
    "        faiss_index = build_faiss_index(\n",
    "            strain_embeddings_reduced, \n",
    "            Config, \n",
    "            logger\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Faiss index building failed: {e}\")\n",
    "        faiss_index = None\n",
    "\n",
    "    # Real-Time Recommendation\n",
    "    try:\n",
    "        if faiss_index is not None:\n",
    "            logger.info(\"Performing real-time recommendation using Faiss...\")\n",
    "            # Select a random user from the validation set\n",
    "            random_user = val_df.sample(1)['user_id_code'].values[0]\n",
    "            logger.info(f\"Selected random user_id_code: {random_user}\")\n",
    "\n",
    "            # Get the user's interactions\n",
    "            user_interactions = train_df[train_df['user_id_code'] == random_user]['strain_id_code'].values\n",
    "            if len(user_interactions) == 0:\n",
    "                logger.warning(f\"No interactions found for user {random_user}. Using the user's average embedding.\")\n",
    "                if random_user < len(user_embeddings_reduced):\n",
    "                    query_embedding = user_embeddings_reduced[random_user].reshape(1, -1).astype(np.float32)\n",
    "                else:\n",
    "                    logger.error(f\"User code {random_user} is out of bounds for user_embeddings_reduced.\")\n",
    "                    query_embedding = np.zeros((1, user_embeddings_reduced.shape[1]), dtype=np.float32)\n",
    "            else:\n",
    "                # Compute the average embedding for the user based on interacted strains\n",
    "                user_emb_als = strain_embeddings_reduced[user_interactions].mean(axis=0)  # Using strain embeddings for consistency\n",
    "                user_emb = user_emb_als.reshape(1, -1).astype(np.float32)\n",
    "                query_embedding = user_emb  # 128-dimensional to match Faiss index\n",
    "\n",
    "            recommended_strains = real_time_recommendation(\n",
    "                query_embedding=query_embedding, \n",
    "                faiss_index=faiss_index, \n",
    "                strain_data=strain_data, \n",
    "                top_k=Config.K, \n",
    "                logger=logger\n",
    "            )\n",
    "\n",
    "            # Log the recommended strain names\n",
    "            logger.info(f\"Recommended strains for user {random_user}: {recommended_strains}\")\n",
    "        else:\n",
    "            logger.error(\"Faiss index is not available. Skipping real-time recommendation.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Real-time recommendation failed: {e}\")\n",
    "\n",
    "    # Evaluate Diversity and Serendipity\n",
    "    try:\n",
    "        if faiss_index is not None and len(recommended_strains) > 0:\n",
    "            logger.info(\"Evaluating diversity and serendipity of recommendations...\")\n",
    "            diversity_score = calculate_diversity(\n",
    "                recommendations=recommended_strains, \n",
    "                strain_embeddings=strain_embeddings_reduced, \n",
    "                strain_data=strain_data, \n",
    "                logger=logger\n",
    "            )\n",
    "            # Get true likes for the user\n",
    "            true_likes = set(train_df[train_df['user_id_code'] == random_user]['strain_id_code'].unique())\n",
    "            serendipity_score = calculate_serendipity(\n",
    "                recommendations=recommended_strains, \n",
    "                true_likes=true_likes, \n",
    "                strain_data=strain_data, \n",
    "                logger=logger\n",
    "            )\n",
    "            logger.info(f\"Diversity Score: {diversity_score:.4f}\")\n",
    "            logger.info(f\"Serendipity Score: {serendipity_score:.4f}\")\n",
    "        else:\n",
    "            logger.warning(\"Cannot evaluate diversity and serendipity without recommendations.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Diversity and serendipity evaluation failed: {e}\")\n",
    "\n",
    "    logger.info(\"Training and evaluation completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "3e3b74fd9bd4f138",
   "execution_count": 44,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
